# ğŸ™ï¸ Realtime Transcript

á»¨ng dá»¥ng web chuyá»ƒn Ä‘á»•i giá»ng nÃ³i thÃ nh vÄƒn báº£n (Speech-to-Text) theo thá»i gian thá»±c vÃ  tá»« file audio/video.

- Há»— trá»£ nhiá»u backend model:
  - **Whisper** (qua `faster-whisper`) â€” máº·c Ä‘á»‹nh, tá»‘i Æ°u realtime, mÃ£ nguá»“n má»Ÿ
  - **SenseVoice** (qua `funasr`) â€” thay tháº¿ Whisper, cÃ³ fallback timestamp
  - **Gemini** (qua Gemini API) â€” Google Gemini 2.5 Flash/Pro, chÃ­nh xÃ¡c cao
  - **Qwen Audio** (qua Modal/vLLM) â€” Alibaba Qwen2-Audio-7B, há»— trá»£ nhiá»u ngÃ´n ngá»¯
  - **Qwen Omni** (qua API) â€” Qwen3-Omni-30B, multimodal audio understanding
  - **Hybrid** - Káº¿t há»£p Whisper (realtime) + Qwen Omni (full transcription)

## âœ¨ TÃ­nh nÄƒng

- **Realtime Transcription**: Chuyá»ƒn Ä‘á»•i giá»ng nÃ³i thÃ nh vÄƒn báº£n theo thá»i gian thá»±c qua WebSocket vá»›i 2 loáº¡i messages:
  - **Partial messages**: Transcript táº¡m thá»i tá»« buffer ~1s (Ä‘á»™ trá»… tháº¥p)
  - **Full messages**: Transcript chÃ­nh xÃ¡c hÆ¡n tá»« toÃ n bá»™ Ä‘oáº¡n speech khi phÃ¡t hiá»‡n káº¿t thÃºc (Ä‘á»™ chÃ­nh xÃ¡c cao)
- **Smart Buffer Management**: Tá»± Ä‘á»™ng tÃ­ch lÅ©y audio vÃ  transcribe láº¡i vá»›i cáº¥u hÃ¬nh tá»‘t hÆ¡n khi phÃ¡t hiá»‡n káº¿t thÃºc Ä‘oáº¡n speech
- **File Upload**: Upload vÃ  transcribe file audio/video (mp3, wav, m4a, mp4, avi, mov, ...), tráº£ vá» full transcript vÃ  danh sÃ¡ch segments cÃ³ timestamp
- **Speaker Detection (Nháº­n diá»‡n ngÆ°á»i nÃ³i)**: TÃ¹y chá»n nháº­n diá»‡n vÃ  phÃ¢n biá»‡t nhiá»u ngÆ°á»i nÃ³i trong audio
  - **Whisper-based:**
    - **main_v3.py** (khuyáº¿n nghá»‹ â­): 2-tier matching (EMA + cluster centroid), persistent memory
    - **main_v2.py**: Bootstrap clustering vá»›i K-means, tá»± Ä‘á»™ng phÃ¢n cá»¥m
    - **main.py**: Chá»‰ há»— trá»£ trong file upload
  - **API-based:** 3-tier matching (EMA â†’ centroid â†’ verification model) cho Ä‘á»™ chÃ­nh xÃ¡c cao
  - Há»— trá»£ cáº¥u hÃ¬nh sá»‘ lÆ°á»£ng ngÆ°á»i nÃ³i tá»‘i Ä‘a (máº·c Ä‘á»‹nh: 2)
  - Speaker ID format: `spk_01`, `spk_02` (Whisper) hoáº·c `SPEAKER_00`, `SPEAKER_01` (v3)
- **Äa ngÃ´n ngá»¯**: Há»— trá»£ nhiá»u ngÃ´n ngá»¯ vá»›i tá»± Ä‘á»™ng phÃ¡t hiá»‡n ngÃ´n ngá»¯
- **Timestamps**: Realtime cÃ³ timestamp theo buffer; Upload cÃ³ timestamp tá»« model hoáº·c Ä‘Æ°á»£c suy Ä‘oÃ¡n (fallback) theo Ä‘á»™ dÃ i ná»™i dung
- **RTF (Real-Time Factor)**: TÃ­nh toÃ¡n vÃ  hiá»ƒn thá»‹ RTF cho file upload Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t xá»­ lÃ½
- **Auto-detect WebSocket URL**: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  cáº¥u hÃ¬nh WebSocket URL tá»« port cá»§a backend
- **Error Handling**: Há»‡ thá»‘ng thÃ´ng bÃ¡o lá»—i/thÃ nh cÃ´ng vá»›i tá»± Ä‘á»™ng dá»«ng khi cÃ³ lá»—i
- **UI Protection**: Tá»± Ä‘á»™ng disable cÃ¡c tÃ¹y chá»n cáº¥u hÃ¬nh khi Ä‘ang xá»­ lÃ½ Ä‘á»ƒ trÃ¡nh thay Ä‘á»•i khÃ´ng mong muá»‘n
- **Drag & Drop**: KÃ©o tháº£ file Ä‘á»ƒ upload dá»… dÃ ng
- **Progress Tracking**: Theo dÃµi tiáº¿n trÃ¬nh upload vÃ  xá»­ lÃ½

## ğŸ“Š So sÃ¡nh cÃ¡c Versions

### Whisper-based Versions (MÃ£ nguá»“n má»Ÿ, tá»‘i Æ°u realtime)

| Feature | main.py | main_v2.py | main_v3.py | main_v4.py | main_v5.py â­ | main_sensevoice.py |
|---------|---------|------------|------------|------------|---------------|-------------------|
| **Model** | Whisper | Whisper | Whisper | Whisper | Whisper | SenseVoice |
| **Realtime Transcription** | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| **File Upload** | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| **Speaker Detection (Realtime)** | âŒ | âœ… Bootstrap | âœ… 2-tier | âœ… 2-tier | âœ… Fusion | âŒ |
| **Speaker Detection (File)** | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| **Embedding System** | - | SpeechBrain | SpeechBrain | Pyannote | **Fusion** | - |
| **Äá»™ phá»©c táº¡p** | ÄÆ¡n giáº£n | Phá»©c táº¡p | Trung bÃ¬nh | Trung bÃ¬nh | Cao | ÄÆ¡n giáº£n |
| **Hiá»‡u suáº¥t** | Tá»‘t | Tá»‘t | Tá»‘t nháº¥t | Ráº¥t tá»‘t | **Tá»‘t nháº¥t** | Tá»‘t |
| **Bootstrap Phase** | - | ~1 phÃºt | KhÃ´ng cáº§n | KhÃ´ng cáº§n | KhÃ´ng cáº§n | - |
| **Speaker Tracking** | File only | Persistent | Persistent | Persistent | Persistent | File only |
| **Chi phÃ­** | Miá»…n phÃ­ | Miá»…n phÃ­ | Miá»…n phÃ­ | Miá»…n phÃ­ | Miá»…n phÃ­ | Miá»…n phÃ­ |
| **Khuyáº¿n nghá»‹** | Testing | Tá»± Ä‘á»™ng phÃ¢n cá»¥m | SpeechBrain | Pyannote | **Fusion - Tá»‘t nháº¥t** | Thay tháº¿ Whisper |

### API-based Versions (ChÃ­nh xÃ¡c cao, yÃªu cáº§u API key/server)

| Feature | main_gemini.py | main_qwenaudio.py | main_qwenomni.py | main_whispersmall_qwenomni.py |
|---------|----------------|-------------------|------------------|-------------------------------|
| **Model** | Gemini 2.5 | Qwen2-Audio-7B | Qwen3-Omni-30B | Whisper + Qwen Omni |
| **Provider** | Google API | Modal/vLLM | Custom API | Local + API |
| **Realtime Transcription** | âœ… Whisper | âœ… Whisper | âœ… Whisper | âœ… Whisper |
| **Full Transcription** | âœ… Gemini | âœ… Qwen Audio | âœ… Qwen Omni | âœ… Qwen Omni |
| **Speaker Detection** | âœ… | âœ… | âœ… | âœ… |
| **Äá»™ chÃ­nh xÃ¡c** | Ráº¥t cao | Cao | Ráº¥t cao | Ráº¥t cao |
| **Latency** | Cao | Trung bÃ¬nh | Trung bÃ¬nh | Trung bÃ¬nh |
| **Chi phÃ­** | CÃ³ phÃ­ | CÃ³ phÃ­ | CÃ³ phÃ­ | CÃ³ phÃ­ |
| **Äa ngÃ´n ngá»¯** | âœ…âœ…âœ… | âœ…âœ… | âœ…âœ…âœ… | âœ…âœ…âœ… |
| **Khuyáº¿n nghá»‹** | ChÃ­nh xÃ¡c nháº¥t | Self-hosted | API endpoint | Hybrid tá»‘t |

**Chá»n version nÃ o?**

**MÃ£ nguá»“n má»Ÿ (Miá»…n phÃ­):**
- ğŸ **Má»›i báº¯t Ä‘áº§u**: `main.py` - ÄÆ¡n giáº£n, dá»… hiá»ƒu
- â­ **Tá»‘t nháº¥t**: `main_v5.py` - Fusion diarization (Pyannote + SpeechBrain), chÃ­nh xÃ¡c cao nháº¥t
- ğŸ¯ **SpeechBrain**: `main_v3.py` - SpeechBrain embeddings, nhanh vÃ  á»•n Ä‘á»‹nh
- ğŸ¨ **Pyannote**: `main_v4.py` - Pyannote embeddings, cháº¥t lÆ°á»£ng cao
- ğŸ”¬ **NghiÃªn cá»©u**: `main_v2.py` - Bootstrap clustering, tá»± Ä‘á»™ng phÃ¢n cá»¥m
- ğŸ”„ **Thay tháº¿**: `main_sensevoice.py` - Model SenseVoice

**API-based (ChÃ­nh xÃ¡c cao, cÃ³ phÃ­):**
- ğŸŒŸ **ChÃ­nh xÃ¡c nháº¥t**: `main_gemini.py` - Google Gemini, há»— trá»£ Ä‘a ngÃ´n ngá»¯ tá»‘t nháº¥t
- ğŸ¢ **Self-hosted**: `main_qwenaudio.py` - Deploy trÃªn Modal/vLLM, kiá»ƒm soÃ¡t data
- ğŸ”Œ **API endpoint**: `main_qwenomni.py` - Qwen3-Omni qua API
- âš¡ **Hybrid**: `main_whispersmall_qwenomni.py` - Káº¿t há»£p tá»‘c Ä‘á»™ + chÃ­nh xÃ¡c

### ğŸ’° So sÃ¡nh Chi phÃ­ & Performance

| Version | Chi phÃ­ | Latency | ChÃ­nh xÃ¡c | GPU Required | Use Case |
|---------|---------|---------|-----------|--------------|----------|
| **main_v5.py** | Miá»…n phÃ­ | Tháº¥p (~150ms) | Ráº¥t cao | Optional | **Production miá»…n phÃ­ tá»‘t nháº¥t â­** |
| **main_v3.py** | Miá»…n phÃ­ | Tháº¥p (~100ms) | Tá»‘t | Optional | Production SpeechBrain |
| **main_v4.py** | Miá»…n phÃ­ | Tháº¥p (~120ms) | Cao | Optional | Production Pyannote |
| **main_gemini.py** | ~$0.01/min | Cao (~1-2s) | Ráº¥t cao | No | Cháº¥t lÆ°á»£ng cao nháº¥t (API) â­ |
| **main_qwenaudio.py** | ~$0.5-1/hour | Trung bÃ¬nh (~500ms) | Cao | Modal GPU | Self-hosted |
| **main_whispersmall_qwenomni.py** | ~$0.005/min | Tháº¥p+Cao | Ráº¥t cao | Optional | Hybrid tá»‘i Æ°u â­ |

**LÆ°u Ã½ vá» chi phÃ­:**
- Whisper versions: HoÃ n toÃ n miá»…n phÃ­, cháº¡y local
- **main_v5.py**: Miá»…n phÃ­, káº¿t há»£p tá»‘t nháº¥t cá»§a Pyannote vÃ  SpeechBrain
- Gemini: Free tier 15 requests/phÃºt, sau Ä‘Ã³ cÃ³ phÃ­
- Qwen Audio Modal: TÃ­nh theo GPU hours (~$0.5-1/hour trÃªn L4/H100)
- Hybrid: Chi phÃ­ tháº¥p hÆ¡n vÃ¬ chá»‰ call API cho full transcription

**Performance Tips:**
- **Tá»‘t nháº¥t (Miá»…n phÃ­)**: DÃ¹ng `main_v5.py` - Fusion diarization vá»›i score-level fusion
- **Äá»™ trá»… tháº¥p**: DÃ¹ng `main_v3.py` - SpeechBrain nhanh nháº¥t
- **ChÃ­nh xÃ¡c cao (Miá»…n phÃ­)**: DÃ¹ng `main_v4.py` hoáº·c `main_v5.py`
- **ChÃ­nh xÃ¡c cao (API)**: DÃ¹ng Gemini hoáº·c Hybrid versions
- **Data privacy**: DÃ¹ng Whisper local versions (v3, v4, v5)

## ğŸ—ï¸ Kiáº¿n trÃºc

- **Backend**: FastAPI (WebSocket cho realtime, REST cho upload)
- **Frontend**: HTML/JavaScript vá»›i WebSocket client
- **Models**: 
  - **MÃ£ nguá»“n má»Ÿ**: Whisper (`faster-whisper`), SenseVoice (`funasr`)
  - **API-based**: Google Gemini, Qwen Audio (Modal/vLLM), Qwen Omni
  - **Hybrid**: Whisper (realtime) + API (full transcription)
- **Speaker Diarization**: 
  - **Fusion**: Káº¿t há»£p Pyannote + SpeechBrain vá»›i nhiá»u fusion methods (main_v5.py â­)
  - **SpeechBrain**: ECAPA-TDNN vá»›i 2-tier matching (EMA + cluster centroid) (main_v3.py)
  - **Pyannote**: pyannote.audio vá»›i 2-tier matching (main_v4.py)
  - Bootstrap clustering vá»›i K-means (main_v2.py)
  - 3-tier matching vá»›i verification model (API versions)
- **Infrastructure**:
  - Local: CUDA 12.1 + cuDNN 9 cho GPU inference
  - Cloud: Modal platform cho Qwen Audio deployment
  - API: Google Gemini API, Custom endpoints cho Qwen Omni

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n má»m
- Python 3.8+
- ffmpeg (Ä‘á»ƒ xá»­ lÃ½ video files)

### Pháº§n cá»©ng
- **CPU**: MÃ¡y tÃ­nh cÃ³ CPU Ä‘á»§ máº¡nh (khuyáº¿n nghá»‹: 4+ cores)
- **GPU**: TÃ¹y chá»n, nhÆ°ng khuyáº¿n nghá»‹ náº¿u muá»‘n xá»­ lÃ½ nhanh hÆ¡n (CUDA compatible)

### CÃ i Ä‘áº·t ffmpeg

**Ubuntu/Debian:**
```bash
sudo apt-get update
sudo apt-get install ffmpeg
```

**macOS:**
```bash
brew install ffmpeg
```

**Windows:**
Táº£i tá»« [ffmpeg.org](https://ffmpeg.org/download.html) vÃ  thÃªm vÃ o PATH

**Conda:**
```bash
conda install -c conda-forge ffmpeg
```

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone repository
```bash
git clone https://github.com/trunghoang2002/realtime-transcript.git
cd realtime-transcript
```

### 2. Táº¡o mÃ´i trÆ°á»ng áº£o (khuyáº¿n nghá»‹)
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate  # Windows
```

### 3. CÃ i Ä‘áº·t dependencies
```bash
cd backend
pip install -r constraints.txt
pip install -r requirements.txt
```

**GPU (CUDA)**
- Khuyáº¿n nghá»‹ CUDA 12.1 + cuDNN 9
- Dá»± Ã¡n cÃ³ sáºµn script Ä‘á»ƒ set mÃ´i trÆ°á»ng CUDA/cuDNN vÃ  cháº¡y server:
  - `backend/scripts/run_main_with_cuda.sh` - Whisper (main.py)
  - `backend/scripts/run_main_v2_with_cuda.sh` - Whisper vá»›i bootstrap speaker detection (main_v2.py)
  - `backend/scripts/run_main_v3_with_cuda.sh` - Whisper vá»›i SpeechBrain diarization (main_v3.py)
  - `backend/scripts/run_main_v4_with_cuda.sh` - Whisper vá»›i Pyannote diarization (main_v4.py)
  - `backend/scripts/run_main_v5_with_cuda.sh` - Whisper vá»›i Fusion diarization (main_v5.py, khuyáº¿n nghá»‹ â­)
  - `backend/scripts/run_main_sensevoice_with_cuda.sh` - SenseVoice

### 4. Cáº¥u hÃ¬nh model (tÃ¹y chá»n)

Chá»‰nh sá»­a file tÆ°Æ¡ng á»©ng Ä‘á»ƒ thay Ä‘á»•i model vÃ  device:
- `backend/main.py` - Whisper version cÆ¡ báº£n
- `backend/main_v2.py` - Whisper vá»›i bootstrap speaker detection
- `backend/main_v3.py` - Whisper vá»›i RealtimeSpeakerDiarization (khuyáº¿n nghá»‹)
- `backend/main_sensevoice.py` - SenseVoice

```python
MODEL_NAME = "small"   # "small" (nhanh), "medium" (chÃ­nh xÃ¡c), "large-v3" (náº·ng)
DEVICE = "cpu"         # "cuda" náº¿u cÃ³ GPU, "cpu" náº¿u khÃ´ng
COMPUTE_TYPE = "int8"  # "float16" trÃªn GPU, "int8" hoáº·c "int8_float16" trÃªn CPU
```

**Khuyáº¿n nghá»‹:**
- Whisper + CPU: `MODEL_NAME = "small"`, `DEVICE = "cpu"`, `COMPUTE_TYPE = "int8"`
- Whisper + GPU: `MODEL_NAME = "medium"`, `DEVICE = "cuda"`, `COMPUTE_TYPE = "float16"`
- SenseVoice + GPU: `DEVICE = "cuda"`
- SenseVoice + CPU: `DEVICE = "cpu"`

**Chá»n version:**
- `main.py`: Version cÆ¡ báº£n, khÃ´ng cÃ³ speaker detection trong realtime
- `main_v2.py`: Version vá»›i bootstrap clustering speaker detection
- `main_v3.py`: Version vá»›i RealtimeSpeakerDiarization class (khuyáº¿n nghá»‹ - Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£)

### 5. Cáº¥u hÃ¬nh Pyannote (cho main_v4.py vÃ  main_v5.py)

**YÃªu cáº§u HuggingFace Token:**
```bash
# Táº¡o .env file trong thÆ° má»¥c backend
echo "HF_TOKEN=your_huggingface_token_here" > backend/.env
```
- Láº¥y token tá»«: https://huggingface.co/settings/tokens
- Accept Ä‘iá»u khoáº£n cá»§a model: https://huggingface.co/pyannote/speaker-diarization-community-1
- File sá»­ dá»¥ng: `main_v4.py`, `main_v5.py`

**LÆ°u Ã½:**
- Pyannote model yÃªu cáº§u accept user agreement trÆ°á»›c
- Token cáº§n cÃ³ quyá»n read
- main_v5.py cÃ³ thá»ƒ cháº¡y vá»›i chá»‰ SpeechBrain náº¿u khÃ´ng cÃ³ token (set `use_pyannote=False`)

### 6. Cáº¥u hÃ¬nh API-based versions (tÃ¹y chá»n)

**Gemini API:**
```bash
# Táº¡o .env file trong thÆ° má»¥c backend
echo "GEMINI_API_KEY=your_api_key_here" > backend/.env
```
- Láº¥y API key tá»«: https://aistudio.google.com/apikey
- File sá»­ dá»¥ng: `main_gemini.py`, `main_v2_gemini.py`

**Qwen Audio (Modal):**
```bash
# Deploy lÃªn Modal
cd backend
modal deploy qwen_audio_modal.py

# Cáº­p nháº­t API_URL trong main_qwenaudio.py
API_URL = "https://your-modal-url/v1/chat/completions"
```
- YÃªu cáº§u: Modal account vÃ  API token
- Chi phÃ­: Theo GPU usage trÃªn Modal platform

**Qwen Omni API:**
```bash
# Cáº­p nháº­t API_URL trong main_qwenomni.py hoáº·c main_whispersmall_qwenomni.py
API_URL = "https://your-qwen-endpoint/v1/chat/completions"
```
- YÃªu cáº§u: Custom API endpoint hoáº·c ngrok tunnel
- File sá»­ dá»¥ng: `main_qwenomni.py`, `main_whispersmall_qwenomni.py`

**LÆ°u Ã½:**
- API-based versions cÃ³ chi phÃ­ sá»­ dá»¥ng
- Gemini: Miá»…n phÃ­ tier cÃ³ giá»›i háº¡n requests/day
- Qwen Audio: Chi phÃ­ GPU trÃªn Modal (~$0.5-1/hour)
- Qwen Omni: TÃ¹y thuá»™c vÃ o hosting solution

## â–¶ï¸ Cháº¡y á»©ng dá»¥ng

### 1. Khá»Ÿi Ä‘á»™ng server (CPU)
```bash
cd backend
python main.py          # Version cÆ¡ báº£n
# hoáº·c
python main_v2.py       # Version vá»›i bootstrap speaker detection
# hoáº·c
python main_v3.py       # Version vá»›i RealtimeSpeakerDiarization (khuyáº¿n nghá»‹)
```

Server sáº½ cháº¡y táº¡i: `http://localhost:8917`

### 2. Cháº¡y vá»›i CUDA (náº¿u cÃ³ GPU)

**YÃªu cáº§u:**
- CUDA 12.1 Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t
- File `~/activate_cuda121.sh` Ä‘á»ƒ activate CUDA 12.1
- Conda environment `v2t` Ä‘Ã£ Ä‘Æ°á»£c táº¡o vÃ  cÃ³ cÃ¡c dependencies

**Cháº¡y server (Whisper-based):**
```bash
cd backend/scripts

# Whisper versions (miá»…n phÃ­, mÃ£ nguá»“n má»Ÿ)
./run_main_with_cuda.sh          # main.py - Whisper cÆ¡ báº£n (port 8917)
./run_main_v2_with_cuda.sh       # main_v2.py - Bootstrap speaker detection (port 8917)
./run_main_v3_with_cuda.sh       # main_v3.py - SpeechBrain diarization (port 8917)
./run_main_v4_with_cuda.sh       # main_v4.py - Pyannote diarization (port 8917)
./run_main_v5_with_cuda.sh       # main_v5.py - Fusion diarization (port 8917, khuyáº¿n nghá»‹ â­)
./run_main_sensevoice_with_cuda.sh  # SenseVoice (port 8918)
```

**Cháº¡y server (API-based):**
```bash
cd backend/scripts

# API-based versions (chÃ­nh xÃ¡c cao, cÃ³ phÃ­)
./run_main_gemini_with_cuda.sh              # Gemini 2.5 (khuyáº¿n nghá»‹ cho chÃ­nh xÃ¡c â­)
./run_main_v2_gemini_with_cuda.sh           # Gemini + Bootstrap clustering
./run_main_qwenaudio_with_cuda.sh           # Qwen Audio (self-hosted)
./run_main_qwenomni_with_cuda.sh            # Qwen Omni (API endpoint)
./run_main_whispersmall_qwenomni_with_cuda.sh  # Hybrid: Whisper + Qwen (khuyáº¿n nghá»‹ cho hybrid â­)
```

**Vá»›i auto-reload (development):**
```bash
# ThÃªm --reload flag vÃ o báº¥t ká»³ script nÃ o
./run_main_with_cuda.sh --reload
./run_main_v3_with_cuda.sh --reload
./run_main_gemini_with_cuda.sh --reload
./run_main_whispersmall_qwenomni_with_cuda.sh --reload
```

**LÆ°u Ã½:**
- Scripts tá»± Ä‘á»™ng:
  - Activate conda environment `v2t`
  - Setup CUDA paths vÃ  cuDNN libraries tá»« conda env
  - Cáº¥u hÃ¬nh `LD_LIBRARY_PATH` cho cuDNN vÃ  PyTorch libraries
  - Sá»­ dá»¥ng GPU device 1 (`CUDA_VISIBLE_DEVICES=1`)
- **Ports:**
  - Whisper versions: port **8917**
  - SenseVoice: port **8918**
  - Táº¥t cáº£ API-based versions: port **8917**
- **Khuyáº¿n nghá»‹:**
  - **Miá»…n phÃ­ tá»‘t nháº¥t**: `main_v5.py` - Fusion diarization (Pyannote + SpeechBrain) â­
  - **Miá»…n phÃ­ nhanh**: `main_v3.py` - SpeechBrain diarization
  - **ChÃ­nh xÃ¡c cao (API)**: `main_gemini.py` - Google Gemini API â­
  - **Hybrid**: `main_whispersmall_qwenomni.py` - CÃ¢n báº±ng tá»‘c Ä‘á»™ & chÃ­nh xÃ¡c â­
- **API-based versions yÃªu cáº§u:**
  - Gemini: `GEMINI_API_KEY` trong `.env` file
  - Qwen Audio: Modal deployment hoáº·c local vLLM server
  - Qwen Omni: Custom API endpoint
- Náº¿u gáº·p lá»—i, kiá»ƒm tra:
  - Conda env `v2t` Ä‘Ã£ Ä‘Æ°á»£c táº¡o chÆ°a
  - File `~/activate_cuda121.sh` cÃ³ tá»“n táº¡i khÃ´ng
  - CUDA 12.1 Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t Ä‘Ãºng chÆ°a
  - API keys/endpoints Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh chÆ°a (cho API versions)

### 3. Truy cáº­p á»©ng dá»¥ng
Má»Ÿ trÃ¬nh duyá»‡t vÃ  truy cáº­p: `http://localhost:8917`

## ğŸ“¡ API Endpoints

### WebSocket: `/ws`

Káº¿t ná»‘i WebSocket Ä‘á»ƒ realtime transcription.

**Protocol:**
1. Client gá»­i message báº¯t Ä‘áº§u:
```json
{
  "event": "start",
  "sample_rate": 16000,
  "format": "pcm16",
  "language": "vi",  // hoáº·c "auto", "en", "ja", ...
  "detect_speaker": false,  // (optional) Báº­t/táº¯t nháº­n diá»‡n ngÆ°á»i nÃ³i
  "max_speakers": 2  // (optional) Sá»‘ lÆ°á»£ng ngÆ°á»i nÃ³i tá»‘i Ä‘a (chá»‰ khi detect_speaker=true)
}
```

2. Client gá»­i audio chunks dÆ°á»›i dáº¡ng binary (PCM16 mono 16kHz)

3. Server tráº£ vá»:
```json
{"type": "ready"}  // Khi sáºµn sÃ ng nháº­n audio
{"type": "partial", "text": "...", "speaker_id": "spk_01", "language": "vi", "language_probability": 0.95, "segments": [{"start": 0.0, "end": 1.0, "text": "...", "speaker_id": "spk_01"}]}  // Transcript táº¡m thá»i tá»« buffer ~1s
{"type": "full", "text": "...", "speaker_id": "spk_01", "language": "vi", "language_probability": 0.95, "segments": [...]}  // Transcript chÃ­nh xÃ¡c tá»« toÃ n bá»™ Ä‘oáº¡n speech (khi phÃ¡t hiá»‡n káº¿t thÃºc)
{"type": "final", "text": ""}  // Khi session káº¿t thÃºc
{"type": "error", "message": "..."}  // Náº¿u cÃ³ lá»—i
{"type": "pong"}  // Pháº£n há»“i cho ping
```

**LÆ°u Ã½ vá» Partial vs Full messages:**
- **Partial**: ÄÆ°á»£c gá»­i liÃªn tá»¥c tá»« buffer ~1s, cÃ³ Ä‘á»™ trá»… tháº¥p nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c cÃ³ thá»ƒ chÆ°a tá»‘i Æ°u
- **Full**: ÄÆ°á»£c gá»­i khi phÃ¡t hiá»‡n káº¿t thÃºc Ä‘oáº¡n speech (silence hoáº·c repeated substring), transcribe láº¡i vá»›i cáº¥u hÃ¬nh tá»‘t hÆ¡n (beam_size=5, best_of=5 cho Whisper hoáº·c batch_size_s=20 cho SenseVoice) Ä‘á»ƒ cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n
- Frontend tá»± Ä‘á»™ng thay tháº¿ pháº§n partial tÆ°Æ¡ng á»©ng báº±ng full text khi nháº­n Ä‘Æ°á»£c full message

4. Client cÃ³ thá»ƒ gá»­i ping Ä‘á»ƒ kiá»ƒm tra káº¿t ná»‘i:
```json
{"event": "ping"}
```
Server sáº½ tráº£ vá» `{"type": "pong"}`

5. Client gá»­i Ä‘á»ƒ dá»«ng:
```json
{"event": "stop"}
```
Khi nháº­n Ä‘Æ°á»£c stop, server sáº½:
- Flush pháº§n cÃ²n láº¡i trong buffer
- Transcribe láº¡i full_buffer náº¿u cÃ²n (náº¿u chÆ°a Ä‘Æ°á»£c gá»­i)
- Gá»­i final message vÃ  Ä‘Ã³ng káº¿t ná»‘i

### REST API: `POST /api/transcribe`

Upload file audio/video Ä‘á»ƒ transcribe.

**Request:**
- Method: `POST`
- Content-Type: `multipart/form-data`
- Form fields:
  - `file`: File audio/video (required)
  - `language`: (optional) NgÃ´n ngá»¯ ("vi", "en", "auto", ...)
  - `detect_speaker`: (optional) "true" hoáº·c "false" - Báº­t/táº¯t nháº­n diá»‡n ngÆ°á»i nÃ³i (máº·c Ä‘á»‹nh: "false")
  - `max_speakers`: (optional) Sá»‘ lÆ°á»£ng ngÆ°á»i nÃ³i tá»‘i Ä‘a (chá»‰ khi detect_speaker="true", máº·c Ä‘á»‹nh: 2)

**Response:**
```json
{
  "success": true,
  "filename": "audio.mp3",
  "text": "Full transcript text...",
  "segments": [
    {
      "start": 0.0,
      "end": 5.2,
      "text": "Äoáº¡n text Ä‘áº§u tiÃªn",
      "speaker_id": "spk_01"  // Chá»‰ cÃ³ khi detect_speaker=true
    },
    ...
  ],
  "language": "vi",
  "language_probability": 0.95,
  "rtf": 0.234  // Real-Time Factor: processing_time / audio_duration
}
```

**Example vá»›i curl:**
```bash
# Upload cÆ¡ báº£n
curl -X POST http://localhost:8917/api/transcribe \
  -F "file=@audio.mp3" \
  -F "language=vi"

# Upload vá»›i speaker detection
curl -X POST http://localhost:8917/api/transcribe \
  -F "file=@audio.mp3" \
  -F "language=vi" \
  -F "detect_speaker=true" \
  -F "max_speakers=3"
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
realtime-transcript/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                          # FastAPI server vá»›i Whisper model (version cÆ¡ báº£n)
â”‚   â”œâ”€â”€ main_v2.py                       # FastAPI server vá»›i Whisper + bootstrap speaker detection
â”‚   â”œâ”€â”€ main_v3.py                       # FastAPI server vá»›i Whisper + SpeechBrain diarization
â”‚   â”œâ”€â”€ main_v4.py                       # FastAPI server vá»›i Whisper + Pyannote diarization
â”‚   â”œâ”€â”€ main_v5.py                       # FastAPI server vá»›i Whisper + Fusion diarization (khuyáº¿n nghá»‹ â­)
â”‚   â”œâ”€â”€ main_sensevoice.py               # FastAPI server vá»›i SenseVoice model
â”‚   â”œâ”€â”€ main_gemini.py                   # FastAPI server vá»›i Gemini model
â”‚   â”œâ”€â”€ main_qwenaudio.py                # FastAPI server vá»›i Qwen Audio
â”‚   â”œâ”€â”€ main_qwenomni.py                 # FastAPI server vá»›i Qwen Omni
â”‚   â”œâ”€â”€ main_whispersmall_qwenomni.py    # FastAPI server vá»›i Whisper Small + Qwen Omni
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt                 # Python dependencies
â”‚   â”œâ”€â”€ constraints.txt                  # Version constraints cho dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/                         # Scripts Ä‘á»ƒ cháº¡y server vá»›i CUDA
â”‚   â”‚   â”œâ”€â”€ activate_cuda_env.sh         # Script activate CUDA environment
â”‚   â”‚   â”œâ”€â”€ run_main_with_cuda.sh        # Cháº¡y main.py vá»›i CUDA
â”‚   â”‚   â”œâ”€â”€ run_main_v2_with_cuda.sh     # Cháº¡y main_v2.py vá»›i CUDA
â”‚   â”‚   â”œâ”€â”€ run_main_v3_with_cuda.sh     # Cháº¡y main_v3.py vá»›i CUDA
â”‚   â”‚   â”œâ”€â”€ run_main_v4_with_cuda.sh     # Cháº¡y main_v4.py vá»›i CUDA
â”‚   â”‚   â”œâ”€â”€ run_main_v5_with_cuda.sh     # Cháº¡y main_v5.py vá»›i CUDA (khuyáº¿n nghá»‹ â­)
â”‚   â”‚   â”œâ”€â”€ run_main_sensevoice_with_cuda.sh  # Cháº¡y SenseVoice vá»›i CUDA
â”‚   â”‚   â”œâ”€â”€ run_main_gemini_with_cuda.sh      # Cháº¡y Gemini vá»›i CUDA
â”‚   â”‚   â””â”€â”€ ...                          # CÃ¡c scripts khÃ¡c
â”‚   â”‚
â”‚   â”œâ”€â”€ docs/                            # TÃ i liá»‡u
â”‚   â”‚   â”œâ”€â”€ whisper_docs.md              # TÃ i liá»‡u vá» Whisper model
â”‚   â”‚   â”œâ”€â”€ sensevoice_docs.md           # TÃ i liá»‡u vá» SenseVoice model
â”‚   â”‚   â””â”€â”€ whisper_vs_sensevoice.md     # So sÃ¡nh Whisper vs SenseVoice
â”‚   â”‚
â”‚   â”œâ”€â”€ get_audio.py                     # Utilities Ä‘á»ƒ decode audio/video files
â”‚   â”œâ”€â”€ silero_vad.py                    # VAD (Voice Activity Detection) sá»­ dá»¥ng Silero VAD
â”‚   â”œâ”€â”€ fix_speechbrain.py               # Patch compatibility cho SpeechBrain vá»›i huggingface_hub
â”‚   â”œâ”€â”€ speechbrain_diarization.py       # RealtimeSpeakerDiarization class vá»›i SpeechBrain ECAPA-TDNN
â”‚   â”œâ”€â”€ pyanote_diarization.py           # RealtimeSpeakerDiarization class vá»›i Pyannote embeddings
â”‚   â”œâ”€â”€ fusion_diarization.py            # Fusion diarization (Pyannote + SpeechBrain) vá»›i nhiá»u fusion methods
â”‚   â”œâ”€â”€ qwen_audio_modal.py              # Qwen Audio model integration
â”‚   â”‚
â”‚   â”œâ”€â”€ check_cuda.py                    # Script kiá»ƒm tra CUDA availability
â”‚   â”œâ”€â”€ test.py                          # Script test
â”‚   â”œâ”€â”€ test_call_qwen_audio.py          # Test Qwen Audio
â”‚   â”‚
â”‚   â”œâ”€â”€ eval/                            # ThÆ° má»¥c evaluation/testing
â”‚   â”‚   â”œâ”€â”€ eval.py                      # Script Ä‘Ã¡nh giÃ¡ model
â”‚   â”‚   â”œâ”€â”€ en/                          # Test cases tiáº¿ng Anh
â”‚   â”‚   â””â”€â”€ ja/                          # Test cases tiáº¿ng Nháº­t
â”‚   â”‚
â”‚   â”œâ”€â”€ pretrained_models/               # ThÆ° má»¥c chá»©a pretrained models
â”‚   â””â”€â”€ __pycache__/                     # Python cache (tá»± Ä‘á»™ng táº¡o)
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html                       # Frontend UI (HTML + JavaScript)
â”‚
â””â”€â”€ README.md                            # TÃ i liá»‡u nÃ y
```

### MÃ´ táº£ cÃ¡c thÃ nh pháº§n chÃ­nh

#### Backend Core Files (Whisper versions)
- **`main.py`**: Server cÆ¡ báº£n sá»­ dá»¥ng Whisper model (`faster-whisper`)
  - Realtime: Dual-buffer strategy vá»›i partial vÃ  full messages
  - File upload: Transcribe vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u cho Ä‘á»™ chÃ­nh xÃ¡c
  - Speaker detection: KhÃ´ng há»— trá»£ trong realtime (chá»‰ file upload)

- **`main_v2.py`**: Server vá»›i bootstrap clustering speaker detection
  - Realtime: Dual-buffer strategy + bootstrap clustering
  - Speaker detection: Bootstrap phase thu tháº­p embeddings â†’ K-means clustering â†’ nháº­n diá»‡n speaker
  - Phá»©c táº¡p hÆ¡n nhÆ°ng cÃ³ kháº£ nÄƒng tá»± Ä‘á»™ng phÃ¢n cá»¥m speakers

- **`main_v3.py`**: Server vá»›i SpeechBrain diarization
  - Realtime: Dual-buffer strategy + SpeechBrain ECAPA-TDNN
  - Speaker detection: 2-tier matching (EMA embedding + cluster centroid)
  - ÄÆ¡n giáº£n, hiá»‡u quáº£, persistent speaker memory
  - Preload model má»™t láº§n, tÃ¡i sá»­ dá»¥ng cho táº¥t cáº£ sessions

- **`main_v4.py`**: Server vá»›i Pyannote diarization
  - Realtime: Dual-buffer strategy + Pyannote embeddings
  - Speaker detection: 2-tier matching vá»›i pyannote.audio
  - Cháº¥t lÆ°á»£ng embeddings cao, Ä‘á»™ chÃ­nh xÃ¡c tá»‘t
  - Session management cho multiple conversations

- **`main_v5.py`**: Server vá»›i Fusion diarization (KHUYáº¾N NGHá»Š â­)
  - Realtime: Dual-buffer strategy + Fusion (Pyannote + SpeechBrain)
  - Speaker detection: Fusion embeddings vá»›i nhiá»u strategies
  - Fusion methods: score_level, concatenate, weighted_average, v.v.
  - Káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a cáº£ hai há»‡ thá»‘ng, chÃ­nh xÃ¡c cao nháº¥t
  - Session management vÃ  dimension alignment tá»± Ä‘á»™ng

- **`main_sensevoice.py`**: Server sá»­ dá»¥ng SenseVoice model (`funasr`)
  - Realtime: TÆ°Æ¡ng tá»± Whisper vá»›i dual-buffer strategy
  - File upload: Há»— trá»£ multi-language detection vÃ  fallback timestamp synthesis

#### Backend Core Files (API-based versions)

- **`main_gemini.py`**: Server sá»­ dá»¥ng Google Gemini 2.5 API
  - Realtime: Whisper (partial) â†’ Gemini API (full transcription)
  - File upload: Gemini API cho toÃ n bá»™ file
  - Speaker detection: 3-tier matching (EMA â†’ centroid â†’ verification model)
  - Æ¯u Ä‘iá»ƒm: ChÃ­nh xÃ¡c ráº¥t cao, há»— trá»£ Ä‘a ngÃ´n ngá»¯ tá»‘t nháº¥t
  - YÃªu cáº§u: `GEMINI_API_KEY` environment variable

- **`main_qwenaudio.py`**: Server sá»­ dá»¥ng Qwen2-Audio-7B qua Modal/vLLM
  - Realtime: Whisper (partial) â†’ Qwen Audio API (full)
  - File upload: Qwen Audio API
  - Speaker detection: 2-tier matching (EMA + verification)
  - Æ¯u Ä‘iá»ƒm: Self-hosted, kiá»ƒm soÃ¡t data, chÃ­nh xÃ¡c cao
  - YÃªu cáº§u: Modal deployment hoáº·c local vLLM server

- **`main_qwenomni.py`**: Server sá»­ dá»¥ng Qwen3-Omni-30B API
  - Realtime: Whisper (partial) â†’ Qwen Omni API (full)
  - File upload: Qwen Omni API
  - Speaker detection: 2-tier matching
  - Æ¯u Ä‘iá»ƒm: Multimodal, chÃ­nh xÃ¡c cao, há»— trá»£ nhiá»u ngÃ´n ngá»¯
  - YÃªu cáº§u: Custom API endpoint

- **`main_whispersmall_qwenomni.py`**: Hybrid version (KHUYáº¾N NGHá»Š cho API â­)
  - Realtime: Whisper Small (nhanh, Ä‘á»™ trá»… tháº¥p)
  - Full transcription: Qwen Omni API (chÃ­nh xÃ¡c cao)
  - Speaker detection: 2-tier matching
  - Æ¯u Ä‘iá»ƒm: CÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  chÃ­nh xÃ¡c, tá»‘t nháº¥t cho production
  - YÃªu cáº§u: Local Whisper + Qwen API endpoint

- **`main_v2_gemini.py`**: Version káº¿t há»£p bootstrap speaker detection + Gemini
  - TÆ°Æ¡ng tá»± main_v2.py nhÆ°ng sá»­ dá»¥ng Gemini cho transcription
  - Bootstrap clustering cho speaker detection
  - ChÃ­nh xÃ¡c ráº¥t cao cho cáº£ transcript vÃ  speaker

#### Speaker Diarization Module
- **`speechbrain_diarization.py`**: RealtimeSpeakerDiarization vá»›i SpeechBrain
  - SpeechBrain ECAPA-TDNN embeddings
  - 2-tier matching: EMA embedding (fast) + cluster centroid (robust)
  - Persistent speaker memory vá»›i exponential moving average
  - Max speakers constraint vá»›i force-assignment
  - Session management cho multiple conversations
  - Preloaded model support Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t

- **`pyanote_diarization.py`**: RealtimeSpeakerDiarization vá»›i Pyannote
  - Pyannote.audio embeddings (high-quality)
  - 2-tier matching: EMA embedding + cluster centroid
  - Session management vÃ  persistent tracking
  - TÃ­ch há»£p vá»›i pyannote/speaker-diarization-community-1
  - Requires HuggingFace token

- **`fusion_diarization.py`**: Fusion Speaker Diarization (â­ Tá»T NHáº¤T)
  - Káº¿t há»£p Pyannote + SpeechBrain embeddings
  - **Fusion methods**:
    - `score_level`: TÃ­nh similarity riÃªng rá»“i káº¿t há»£p (khuyáº¿n nghá»‹)
    - `concatenate`: [E1 ; E2]
    - `normalized_average`: (norm(E1) + norm(E2)) / 2
    - `weighted_average`: Î±*E1 + (1-Î±)*E2
    - `product`: norm(E1) âŠ™ norm(E2)
    - `max_pool`: max(E1, E2)
    - `learned_concat`: [w1*E1 ; w2*E2]
  - Dimension alignment tá»± Ä‘á»™ng (min/max/pad_zero)
  - Session management vÃ  NaN handling
  - ChÃ­nh xÃ¡c cao nháº¥t, táº­n dá»¥ng cáº£ hai há»‡ thá»‘ng

#### Utility Modules
- **`get_audio.py`**: Xá»­ lÃ½ decode audio/video files thÃ nh numpy array (16kHz mono) sá»­ dá»¥ng `av` (PyAV)
- **`silero_vad.py`**: Voice Activity Detection Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c Ä‘oáº¡n cÃ³ giá»ng nÃ³i, loáº¡i bá» im láº·ng
- **`fix_speechbrain.py`**: Patch Ä‘á»ƒ fix compatibility issue giá»¯a SpeechBrain vÃ  huggingface_hub (cáº§n cho speaker detection)
- **`qwen_audio_modal.py`**: Modal deployment script cho Qwen2-Audio-7B
  - Tá»± Ä‘á»™ng deploy Qwen Audio model lÃªn Modal platform
  - Sá»­ dá»¥ng vLLM inference engine (GPU L4/H100)
  - Auto-scaling vá»›i scaledown window 15 phÃºt
  - OpenAI-compatible API endpoint
  - Há»— trá»£ audio streaming vÃ  batch processing

#### CUDA Scripts (trong `scripts/`)

**Whisper-based:**
- **`run_main_with_cuda.sh`**: Script tá»± Ä‘á»™ng setup CUDA/cuDNN vÃ  cháº¡y main.py trÃªn port 8917
- **`run_main_v2_with_cuda.sh`**: Script cháº¡y main_v2.py (bootstrap speaker detection)
- **`run_main_v3_with_cuda.sh`**: Script cháº¡y main_v3.py (SpeechBrain diarization)
- **`run_main_v4_with_cuda.sh`**: Script cháº¡y main_v4.py (Pyannote diarization)
- **`run_main_v5_with_cuda.sh`**: Script cháº¡y main_v5.py (Fusion diarization, khuyáº¿n nghá»‹ â­)
- **`run_main_sensevoice_with_cuda.sh`**: Script cháº¡y SenseVoice server trÃªn port 8918

**API-based:**
- **`run_main_gemini_with_cuda.sh`**: Script cháº¡y main_gemini.py (Gemini API)
- **`run_main_v2_gemini_with_cuda.sh`**: Script cháº¡y main_v2_gemini.py (Bootstrap + Gemini)
- **`run_main_qwenaudio_with_cuda.sh`**: Script cháº¡y main_qwenaudio.py (Qwen Audio)
- **`run_main_qwenomni_with_cuda.sh`**: Script cháº¡y main_qwenomni.py (Qwen Omni)
- **`run_main_whispersmall_qwenomni_with_cuda.sh`**: Script cháº¡y main_whispersmall_qwenomni.py (Hybrid, khuyáº¿n nghá»‹ â­)

**Helper scripts:**
- **`activate_cuda_env.sh`**: Script helper Ä‘á»ƒ activate CUDA environment
- **`check_cuda.py`**: Script kiá»ƒm tra CUDA cÃ³ sáºµn vÃ  hoáº¡t Ä‘á»™ng khÃ´ng

**Táº¥t cáº£ scripts:**
- Tá»± Ä‘á»™ng activate conda environment `v2t`
- Setup CUDA 12.1 paths vÃ  cuDNN libraries
- Cáº¥u hÃ¬nh `LD_LIBRARY_PATH` cho cuDNN vÃ  PyTorch
- Sá»­ dá»¥ng GPU device 1 (`CUDA_VISIBLE_DEVICES=1`)
- Há»— trá»£ `--reload` flag cho development mode

#### Documentation (trong `docs/`)
- **`whisper_docs.md`**: TÃ i liá»‡u chi tiáº¿t vá» cÃ¡ch sá»­ dá»¥ng Whisper model
- **`sensevoice_docs.md`**: TÃ i liá»‡u chi tiáº¿t vá» cÃ¡ch sá»­ dá»¥ng SenseVoice model
- **`whisper_vs_sensevoice.md`**: So sÃ¡nh Æ°u/nhÆ°á»£c Ä‘iá»ƒm giá»¯a 2 models

#### Evaluation (trong `eval/`)
- **`eval/`**: ThÆ° má»¥c chá»©a scripts vÃ  test cases Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng transcription
  - `eval.py`: Script cháº¡y evaluation
  - `en/`, `ja/`: Test cases cho cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau

## ğŸ¯ CÃ¡ch sá»­ dá»¥ng

### Realtime Transcription

1. Má»Ÿ tab **"Realtime"**
2. Cáº¥u hÃ¬nh:
   - **WebSocket URL**: Tá»± Ä‘á»™ng detect tá»« backend (cÃ³ thá»ƒ chá»‰nh sá»­a náº¿u cáº§n)
   - **NgÃ´n ngá»¯**: Chá»n ngÃ´n ngá»¯ (hoáº·c "Tá»± Ä‘á»™ng")
   - **Detect speaker**: Báº­t/táº¯t nháº­n diá»‡n ngÆ°á»i nÃ³i (tÃ¹y chá»n)
   - **Max speaker**: Sá»‘ lÆ°á»£ng ngÆ°á»i nÃ³i tá»‘i Ä‘a (chá»‰ hiá»‡n khi báº­t Detect speaker, máº·c Ä‘á»‹nh: 2)
3. Nháº¥n **"Start"** vÃ  cho phÃ©p truy cáº­p microphone
   - CÃ¡c tÃ¹y chá»n cáº¥u hÃ¬nh sáº½ tá»± Ä‘á»™ng bá»‹ disable khi Ä‘ang xá»­ lÃ½
4. Báº¯t Ä‘áº§u nÃ³i, transcript sáº½ hiá»ƒn thá»‹ theo thá»i gian thá»±c theo 2 pháº§n:
   - **Full Transcript**: 
     - Hiá»ƒn thá»‹ partial text táº¡m thá»i tá»« buffer ~1s (Ä‘á»™ trá»… tháº¥p)
     - Tá»± Ä‘á»™ng Ä‘Æ°á»£c thay tháº¿ báº±ng full text chÃ­nh xÃ¡c hÆ¡n khi phÃ¡t hiá»‡n káº¿t thÃºc Ä‘oáº¡n speech
     - CÃ³ speaker ID náº¿u báº­t detect speaker
   - **Segments**: Danh sÃ¡ch cÃ¡c Ä‘oáº¡n cÃ³ timestamp (start â†’ end) vÃ  speaker ID (náº¿u cÃ³)
     - Segments tá»« partial messages Ä‘Æ°á»£c thÃªm vÃ o liÃªn tá»¥c
     - Segments tá»« full messages cÃ³ thá»ƒ cáº­p nháº­t/thay tháº¿ segments tÆ°Æ¡ng á»©ng
5. Nháº¥n **"Stop"** Ä‘á»ƒ dá»«ng
   - CÃ¡c tÃ¹y chá»n cáº¥u hÃ¬nh sáº½ Ä‘Æ°á»£c enable láº¡i

**LÆ°u Ã½**: Náº¿u cÃ³ lá»—i xáº£y ra, há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng dá»«ng vÃ  hiá»ƒn thá»‹ thÃ´ng bÃ¡o lá»—i.

### Upload File

1. Má»Ÿ tab **"Upload File"**
2. Cáº¥u hÃ¬nh:
   - **NgÃ´n ngá»¯**: Chá»n ngÃ´n ngá»¯ (hoáº·c "Tá»± Ä‘á»™ng")
   - **Detect speaker**: Báº­t/táº¯t nháº­n diá»‡n ngÆ°á»i nÃ³i (tÃ¹y chá»n)
   - **Max speaker**: Sá»‘ lÆ°á»£ng ngÆ°á»i nÃ³i tá»‘i Ä‘a (chá»‰ hiá»‡n khi báº­t Detect speaker, máº·c Ä‘á»‹nh: 2)
3. KÃ©o tháº£ file vÃ o vÃ¹ng upload hoáº·c click **"Chá»n File"**
   - CÃ¡c tÃ¹y chá»n cáº¥u hÃ¬nh sáº½ tá»± Ä‘á»™ng bá»‹ disable khi Ä‘ang xá»­ lÃ½
4. Chá» xá»­ lÃ½:
   - Progress bar sáº½ hiá»ƒn thá»‹ tiáº¿n trÃ¬nh upload
   - ThÃ´ng bÃ¡o thÃ nh cÃ´ng/lá»—i sáº½ xuáº¥t hiá»‡n á»Ÿ gÃ³c trÃªn bÃªn pháº£i
5. Xem káº¿t quáº£:
   - **Full Transcript**: ToÃ n bá»™ ná»™i dung (cÃ³ speaker ID náº¿u báº­t detect speaker)
   - **Segments**: Danh sÃ¡ch cÃ¡c Ä‘oáº¡n cÃ³ timestamp vÃ  speaker ID (náº¿u cÃ³)
   - **RTF**: Real-Time Factor (hiá»‡u suáº¥t xá»­ lÃ½) - RTF < 1.0 nghÄ©a lÃ  xá»­ lÃ½ nhanh hÆ¡n thá»i gian thá»±c

## ğŸ”€ Fusion Methods (main_v5.py)

Fusion diarization káº¿t há»£p embeddings tá»« Pyannote vÃ  SpeechBrain Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n. Há»— trá»£ nhiá»u fusion strategies:

### Fusion Methods

**1. Score-level Fusion (Khuyáº¿n nghá»‹ â­)**
```python
fusion_method="score_level"
fusion_alpha=0.4  # Weight: 0.4*Pyannote + 0.6*SpeechBrain
```
- TÃ­nh similarity riÃªng cho má»—i há»‡ thá»‘ng, sau Ä‘Ã³ káº¿t há»£p scores
- Formula: `final_score = Î±*score_pyannote + (1-Î±)*score_speechbrain`
- Æ¯u Ä‘iá»ƒm: Linh hoáº¡t, táº­n dá»¥ng tháº¿ máº¡nh cá»§a tá»«ng há»‡ thá»‘ng
- Use case: Khi muá»‘n Ä‘iá»u chá»‰nh trá»ng sá»‘ giá»¯a hai há»‡ thá»‘ng

**2. Normalized Average**
```python
fusion_method="normalized_average"
```
- Average cá»§a normalized embeddings: `E = (norm(E1) + norm(E2)) / 2`
- Æ¯u Ä‘iá»ƒm: ÄÆ¡n giáº£n, cÃ¢n báº±ng giá»¯a hai há»‡ thá»‘ng
- Use case: Khi hai há»‡ thá»‘ng cÃ³ Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘Æ°Æ¡ng

**3. Weighted Average**
```python
fusion_method="weighted_average"
fusion_alpha=0.5  # Weight for Pyannote
```
- Weighted average: `E = Î±*norm(E1) + (1-Î±)*norm(E2)`
- Æ¯u Ä‘iá»ƒm: Äiá»u chá»‰nh Ä‘Æ°á»£c trá»ng sá»‘
- Use case: Khi muá»‘n Æ°u tiÃªn má»™t há»‡ thá»‘ng hÆ¡n

**4. Concatenate**
```python
fusion_method="concatenate"
```
- Simple concatenation: `E = [E1 ; E2]`
- Æ¯u Ä‘iá»ƒm: Giá»¯ nguyÃªn thÃ´ng tin tá»« cáº£ hai
- Use case: Khi cáº§n táº¥t cáº£ features tá»« cáº£ hai há»‡ thá»‘ng

**5. Product**
```python
fusion_method="product"
```
- Element-wise product: `E = norm(E1) âŠ™ norm(E2)`
- Æ¯u Ä‘iá»ƒm: Nháº¥n máº¡nh features chung
- Use case: Khi muá»‘n lá»c noise

**6. Max Pool**
```python
fusion_method="max_pool"
```
- Max pooling: `E = max(norm(E1), norm(E2))`
- Æ¯u Ä‘iá»ƒm: Chá»n features máº¡nh nháº¥t
- Use case: Khi muá»‘n robust vá»›i outliers

**7. Learned Concat**
```python
fusion_method="learned_concat"
fusion_weights=(1.2, 0.8)  # Custom weights
```
- Weighted concatenation: `E = [w1*E1 ; w2*E2]`
- Æ¯u Ä‘iá»ƒm: TÃ¹y chá»‰nh weights cho tá»«ng embedding
- Use case: Sau khi Ä‘Ã£ há»c weights tá»‘i Æ°u

### Dimension Alignment

Pyannote (256-dim) vÃ  SpeechBrain (192-dim) cÃ³ dimensions khÃ¡c nhau. Fusion diarization tá»± Ä‘á»™ng xá»­ lÃ½:

```python
dimension_alignment="min"     # Truncate to 192-dim (máº·c Ä‘á»‹nh, nhanh)
dimension_alignment="max"     # Pad zeros to 256-dim (giá»¯ táº¥t cáº£ info)
dimension_alignment="pad_zero"  # TÆ°Æ¡ng tá»± max
```

### Configuration Example

```python
diarization_model = RealtimeSpeakerDiarization(
    fusion_method="score_level",      # Fusion strategy
    fusion_alpha=0.4,                  # Weight (0.4 Pyannote, 0.6 SpeechBrain)
    dimension_alignment="max",         # Dimension handling
    similarity_threshold=0.6,          # Matching threshold
    embedding_update_weight=0.3,       # EMA update weight
    min_similarity_gap=0.25,           # Minimum gap for matching
    use_pyannote=True,                 # Enable Pyannote
    use_speechbrain=True,              # Enable SpeechBrain
    pyannote_config={
        "model_name": "pyannote/speaker-diarization-community-1",
        "token": os.getenv("HF_TOKEN"),
    },
    speechbrain_config={
        "model_name": "speechbrain/spkrec-ecapa-tdnn-voxceleb",
    }
)
```

## âš™ï¸ Cáº¥u hÃ¬nh nÃ¢ng cao

### Thay Ä‘á»•i port

Sá»­a trong `backend/main.py`:
```python
uvicorn.run("main:app", host="0.0.0.0", port=8917, reload=True)
```

### Thay Ä‘á»•i WebSocket URL

WebSocket URL tá»± Ä‘á»™ng detect tá»« port cá»§a backend. Náº¿u cáº§n chá»‰nh sá»­a thá»§ cÃ´ng, sá»­a trong `frontend/index.html`:
```html
<input id="wsUrl" type="text" value="ws://localhost:8917/ws">
```

Hoáº·c chá»‰nh sá»­a function `getWebSocketUrl()` trong JavaScript Ä‘á»ƒ thay Ä‘á»•i logic auto-detect.

### Tá»‘i Æ°u hÃ³a cho realtime

**Partial transcription** (trong `run_transcribe_on_buffer()`):
- `beam_size=1`: Tá»‘i Æ°u tá»‘c Ä‘á»™ cho streaming
- `best_of=1`: Tá»‘i Æ°u tá»‘c Ä‘á»™
- `condition_on_previous_text=False`: Giáº£m Ä‘á»™ trá»…, cÃ¡c chunk Ä‘á»™c láº­p

**Full transcription** (trong `run_transcribe_on_full_buffer()`):
- Whisper: `beam_size=5`, `best_of=5`: TÄƒng Ä‘á»™ chÃ­nh xÃ¡c khi transcribe láº¡i toÃ n bá»™ Ä‘oáº¡n speech
- SenseVoice: `batch_size_s=20`: TÄƒng batch size Ä‘á»ƒ chÃ­nh xÃ¡c hÆ¡n
- ÄÆ°á»£c gá»i tá»± Ä‘á»™ng khi phÃ¡t hiá»‡n káº¿t thÃºc Ä‘oáº¡n speech (silence hoáº·c repeated substring)

### Tá»‘i Æ°u hÃ³a cho file upload

Trong `transcribe_file()`:
- Whisper: `beam_size=5`, `best_of=5`: TÄƒng Ä‘á»™ chÃ­nh xÃ¡c
- SenseVoice: `batch_size_s=20`: TÄƒng batch size
- `condition_on_previous_text=True` (Whisper): Sá»­ dá»¥ng ngá»¯ cáº£nh

## ğŸ› Troubleshooting

### Lá»—i: "max() arg is an empty sequence"
- **NguyÃªn nhÃ¢n**: Audio quÃ¡ ngáº¯n hoáº·c hoÃ n toÃ n im láº·ng
- **Giáº£i phÃ¡p**: ÄÃ£ Ä‘Æ°á»£c xá»­ lÃ½ tá»± Ä‘á»™ng, chá»‰ cáº§n thá»­ láº¡i

### Lá»—i: "ffmpeg not found"
- **NguyÃªn nhÃ¢n**: ChÆ°a cÃ i Ä‘áº·t ffmpeg
- **Giáº£i phÃ¡p**: CÃ i Ä‘áº·t ffmpeg theo hÆ°á»›ng dáº«n á»Ÿ trÃªn

### Realtime transcription cháº­m
- **Giáº£i phÃ¡p**: 
  - Giáº£m `MODEL_NAME` xuá»‘ng "small"
  - Sá»­ dá»¥ng GPU náº¿u cÃ³
  - TÄƒng `CHUNK_TARGET_BYTES` Ä‘á»ƒ giáº£m sá»‘ láº§n transcribe

### File upload khÃ´ng hoáº¡t Ä‘á»™ng
- **Kiá»ƒm tra**: Äáº£m báº£o file khÃ´ng quÃ¡ lá»›n (giá»›i háº¡n bá»™ nhá»›)
- **Giáº£i phÃ¡p**: Sá»­ dá»¥ng file nhá» hÆ¡n hoáº·c tÄƒng bá»™ nhá»›

### WebSocket connection failed
- **Kiá»ƒm tra**: Äáº£m báº£o server Ä‘ang cháº¡y
- **Kiá»ƒm tra**: URL WebSocket Ä‘Ãºng (ws://localhost:8917/ws)

### CUDA/cuDNN khÃ´ng tÃ¬m tháº¥y (libcudnn_ops.so.*)
- DÃ¹ng script (trong thÆ° má»¥c `backend/scripts/`):
  - `run_main_with_cuda.sh` - main.py
  - `run_main_v2_with_cuda.sh` - main_v2.py
  - `run_main_v3_with_cuda.sh` - main_v3.py (khuyáº¿n nghá»‹)
  - `run_main_sensevoice_with_cuda.sh` - SenseVoice
- Hoáº·c tá»± set:
  ```bash
  export LD_LIBRARY_PATH="$CONDA_PREFIX/lib/python3.10/site-packages/nvidia/cudnn/lib:$CONDA_PREFIX/lib/python3.10/site-packages/torch/lib:$LD_LIBRARY_PATH"
  ```

### Speaker detection khÃ´ng hoáº¡t Ä‘á»™ng tá»‘t
- **main_v2.py**: Cáº§n Ä‘á»£i bootstrap phase (~1 phÃºt) Ä‘á»ƒ thu tháº­p Ä‘á»§ dá»¯ liá»‡u
- **main_v3.py**: SpeechBrain - Hoáº¡t Ä‘á»™ng ngay tá»« Ä‘áº§u, khÃ´ng cáº§n bootstrap
- **main_v4.py**: Pyannote - YÃªu cáº§u HF_TOKEN, cháº¥t lÆ°á»£ng cao
- **main_v5.py** (khuyáº¿n nghá»‹ â­): Fusion - Tá»‘t nháº¥t, káº¿t há»£p cáº£ hai há»‡ thá»‘ng
- Kiá»ƒm tra:
  - `max_speakers` cÃ³ Ä‘Æ°á»£c set Ä‘Ãºng khÃ´ng
  - Audio cÃ³ Ä‘á»§ rÃµ Ä‘á»ƒ trÃ­ch xuáº¥t embedding khÃ´ng
  - Log cÃ³ hiá»ƒn thá»‹ similarity scores khÃ´ng
  - HF_TOKEN cÃ³ Ä‘Æ°á»£c set Ä‘Ãºng cho main_v4/v5 khÃ´ng
  - Embeddings cÃ³ bá»‹ zero vectors khÃ´ng (check warnings)

### Chá»n version nÃ o?
- **Má»›i báº¯t Ä‘áº§u hoáº·c testing**: `main.py` - ÄÆ¡n giáº£n nháº¥t
- **Cáº§n speaker detection tá»‘t nháº¥t (miá»…n phÃ­)**: `main_v5.py` â­ - Fusion diarization, chÃ­nh xÃ¡c cao nháº¥t
- **Cáº§n speaker detection nhanh**: `main_v3.py` - SpeechBrain, nhanh vÃ  á»•n Ä‘á»‹nh
- **Cáº§n speaker detection cháº¥t lÆ°á»£ng cao**: `main_v4.py` - Pyannote embeddings
- **Cáº§n tá»± Ä‘á»™ng phÃ¢n cá»¥m speakers**: `main_v2.py` - Bootstrap clustering
- **Cáº§n chÃ­nh xÃ¡c cao nháº¥t (API)**: `main_gemini.py` â­ - Google Gemini API
- **Cáº§n self-hosted API**: `main_qwenaudio.py` - Deploy trÃªn Modal
- **Cáº§n hybrid tá»‘i Æ°u**: `main_whispersmall_qwenomni.py` â­ - Tá»‘c Ä‘á»™ + chÃ­nh xÃ¡c

### API khÃ´ng hoáº¡t Ä‘á»™ng
**Gemini API:**
- Kiá»ƒm tra `GEMINI_API_KEY` trong `.env` file
- Verify API key táº¡i: https://aistudio.google.com/apikey
- Kiá»ƒm tra quota limits (free tier cÃ³ giá»›i háº¡n)
- Log error Ä‘á»ƒ xem chi tiáº¿t lá»—i tá»« API

**Qwen Audio (Modal):**
- Verify Modal deployment: `modal app list`
- Kiá»ƒm tra URL trong `API_URL` variable
- Test endpoint: `curl https://your-modal-url/health`
- Kiá»ƒm tra Modal logs: `modal app logs qwen-audio-modal`

**Qwen Omni:**
- Kiá»ƒm tra API endpoint Ä‘ang cháº¡y
- Verify API_URL Ä‘Ãºng trong code
- Test vá»›i curl/Postman trÆ°á»›c
- Kiá»ƒm tra network/firewall náº¿u dÃ¹ng local endpoint

### Modal deployment issues
```bash
# Install Modal CLI
pip install modal

# Login
modal token set --token-id YOUR_TOKEN_ID --token-secret YOUR_TOKEN_SECRET

# Deploy
cd backend
modal deploy qwen_audio_modal.py

# Test
modal run qwen_audio_modal.py

# Check logs
modal app logs qwen-audio-modal

# Check status
modal app list
```

## ğŸ“ Ghi chÃº

### Vá» Models vÃ  Versions
- Model Whisper/SenseVoice Ä‘Æ°á»£c táº£i tá»± Ä‘á»™ng láº§n Ä‘áº§u cháº¡y
- Speaker diarization models Ä‘Æ°á»£c preload má»™t láº§n Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t:
  - `main_v3.py`: SpeechBrain ECAPA-TDNN
  - `main_v4.py`: Pyannote.audio embeddings
  - `main_v5.py`: Cáº£ Pyannote vÃ  SpeechBrain (fusion)
- File táº¡m sáº½ tá»± Ä‘á»™ng xÃ³a sau khi xá»­ lÃ½ xong
- **Khuyáº¿n nghá»‹ sá»­ dá»¥ng `main_v5.py`** - Version tá»‘i Æ°u nháº¥t vá»›i Fusion diarization (Pyannote + SpeechBrain)

### Realtime Transcription
- **Dual-buffer strategy**:
  - **recv_buffer**: Buffer ~1s Ä‘á»ƒ transcribe nhanh vÃ  gá»­i partial messages (Ä‘á»™ trá»… tháº¥p)
  - **full_buffer**: TÃ­ch lÅ©y toÃ n bá»™ audio cá»§a má»™t Ä‘oáº¡n speech, Ä‘Æ°á»£c transcribe láº¡i vá»›i cáº¥u hÃ¬nh tá»‘t hÆ¡n khi phÃ¡t hiá»‡n káº¿t thÃºc
  - Káº¿t thÃºc Ä‘oáº¡n speech Ä‘Æ°á»£c phÃ¡t hiá»‡n báº±ng: silence detection hoáº·c repeated substring detection
- **Partial vs Full Messages**:
  - Partial messages: ÄÆ°á»£c gá»­i liÃªn tá»¥c tá»« buffer ~1s, cÃ³ Ä‘á»™ trá»… tháº¥p
  - Full messages: ÄÆ°á»£c gá»­i khi phÃ¡t hiá»‡n káº¿t thÃºc Ä‘oáº¡n speech, cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n
  - Frontend tá»± Ä‘á»™ng thay tháº¿ pháº§n partial tÆ°Æ¡ng á»©ng báº±ng full text Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng transcript
- **Repeated Substring Detection**: Há»‡ thá»‘ng tá»± Ä‘á»™ng phÃ¡t hiá»‡n chuá»—i láº·p láº¡i (â‰¥5 láº§n) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh káº¿t thÃºc Ä‘oáº¡n speech vÃ  trigger full transcription

### Speaker Detection
- **main.py**: KhÃ´ng há»— trá»£ speaker detection trong realtime (chá»‰ file upload)
- **main_v2.py**: Bootstrap clustering approach
  - Bootstrap phase: Thu tháº­p embeddings trong ~1 phÃºt Ä‘áº§u
  - K-means clustering: PhÃ¢n cá»¥m speakers tá»± Ä‘á»™ng
  - 3-tier matching: EMA embedding â†’ cluster centroid â†’ verification model
  - Phá»©c táº¡p hÆ¡n nhÆ°ng cÃ³ kháº£ nÄƒng tá»± Ä‘á»™ng phÃ¢n cá»¥m
- **main_v3.py**: SpeechBrain diarization
  - SpeechBrain ECAPA-TDNN embeddings
  - 2-tier matching: EMA embedding (fast) + cluster centroid (robust)
  - Persistent speaker memory vÃ  session management
  - Nhanh, á»•n Ä‘á»‹nh, Ä‘á»™ chÃ­nh xÃ¡c tá»‘t
- **main_v4.py**: Pyannote diarization
  - Pyannote.audio embeddings (high-quality)
  - 2-tier matching: EMA embedding + cluster centroid
  - Session management vÃ  persistent tracking
  - Cháº¥t lÆ°á»£ng embeddings cao, Ä‘á»™ chÃ­nh xÃ¡c ráº¥t tá»‘t
- **main_v5.py**: Fusion diarization (KHUYáº¾N NGHá»Š â­)
  - **Káº¿t há»£p cáº£ Pyannote vÃ  SpeechBrain**
  - **Fusion methods**: score_level (khuyáº¿n nghá»‹), concatenate, weighted_average, product, max_pool, learned_concat
  - **2-tier matching** vá»›i fused embeddings
  - **Persistent speaker memory**: LÆ°u trá»¯ cáº£ fused embeddings vÃ  individual embeddings
  - **Dimension alignment**: Tá»± Ä‘á»™ng xá»­ lÃ½ dimension mismatch (Pyannote 256-dim vs SpeechBrain 192-dim)
  - **Max speakers constraint**: Force-assign vÃ o speaker cÃ³ similarity cao nháº¥t khi Ä‘áº¡t limit
  - **Session management**: Multiple conversations Ä‘á»™c láº­p
  - **NaN handling**: Xá»­ lÃ½ zero vectors vÃ  NaN similarities
  - **Preloaded models**: Load cáº£ hai models má»™t láº§n, tÃ¡i sá»­ dá»¥ng cho táº¥t cáº£ sessions
  - Speaker ID Ä‘Æ°á»£c gÃ¡n dáº¡ng `SPEAKER_00`, `SPEAKER_01`, ...
  - **ChÃ­nh xÃ¡c cao nháº¥t**, táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ hai há»‡ thá»‘ng
- Vá»›i video files, audio sáº½ Ä‘Æ°á»£c extract tá»± Ä‘á»™ng náº¿u cÃ³ ffmpeg

### Performance Metrics
- **RTF (Real-Time Factor)**: 
  - RTF = processing_time / audio_duration
  - RTF < 1.0: Xá»­ lÃ½ nhanh hÆ¡n thá»i gian thá»±c (tá»‘t)
  - RTF = 1.0: Xá»­ lÃ½ báº±ng thá»i gian thá»±c
  - RTF > 1.0: Xá»­ lÃ½ cháº­m hÆ¡n thá»i gian thá»±c

### UI/UX Features
- CÃ¡c tÃ¹y chá»n cáº¥u hÃ¬nh tá»± Ä‘á»™ng bá»‹ disable khi Ä‘ang xá»­ lÃ½ Ä‘á»ƒ trÃ¡nh thay Ä‘á»•i khÃ´ng mong muá»‘n
- Há»‡ thá»‘ng tá»± Ä‘á»™ng hiá»ƒn thá»‹/áº©n cÃ¡c pháº§n tá»­ UI dá»±a trÃªn tráº¡ng thÃ¡i (chá»‰ hiá»‡n transcript khi Ä‘ang ghi)
- WebSocket há»— trá»£ ping/pong Ä‘á»ƒ kiá»ƒm tra káº¿t ná»‘i
- Auto-detect WebSocket URL tá»« port cá»§a backend

### Technical Details
- **Speaker tracking Ä‘Æ°á»£c reset cho má»—i session** (WebSocket connection hoáº·c file upload)
- **Session cleanup**: Tá»± Ä‘á»™ng xÃ³a session data Ä‘á»ƒ trÃ¡nh memory leak
- **VAD (Voice Activity Detection)**: Sá»­ dá»¥ng Silero VAD Ä‘á»ƒ lá»c silence
- **Audio format**: PCM16 mono 16kHz cho WebSocket, tá»± Ä‘á»™ng convert cho file upload

## ğŸ”§ Dependencies

Xem `backend/requirements.txt` Ä‘á»ƒ biáº¿t danh sÃ¡ch Ä‘áº§y Ä‘á»§.

**Core Dependencies (Táº¥t cáº£ versions):**
- `fastapi`: Web framework
- `uvicorn`: ASGI server
- `numpy`: Xá»­ lÃ½ audio
- `soundfile`: Audio I/O
- `av` (PyAV): Decode audio/video files
- `torch`: Deep learning framework
- `speechbrain`: Speaker diarization (ECAPA-TDNN) - main_v3.py, main_v5.py
- `pyannote.audio`: Speaker diarization (embeddings) - main_v4.py, main_v5.py
- `scipy`: Scientific computing (cdist for similarity)

**Whisper-based Versions:**
- `faster-whisper`: Whisper backend (main.py, main_v2.py, main_v3.py)
- `funasr`: SenseVoice backend (main_sensevoice.py)
- `silero-vad`: Voice Activity Detection

**API-based Versions:**
- `google-genai`: Google Gemini API client (main_gemini.py, main_v2_gemini.py)
- `requests`: HTTP client (main_qwenaudio.py, main_qwenomni.py, main_whispersmall_qwenomni.py)
- `python-dotenv`: Environment variables management
- `aiohttp`: Async HTTP client (optional)

**Modal Deployment:**
- `modal`: Modal platform SDK (qwen_audio_modal.py)
- YÃªu cáº§u Modal account vÃ  token

**Development:**
- `pytest`: Testing framework
- `black`: Code formatter
- `flake8`: Linter

## ğŸ“„ License

Dá»± Ã¡n nÃ y sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n mÃ£ nguá»“n má»Ÿ. Vui lÃ²ng xem license cá»§a tá»«ng thÆ° viá»‡n.

## ğŸ¤ ÄÃ³ng gÃ³p

Má»i Ä‘Ã³ng gÃ³p Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n! Vui lÃ²ng táº¡o issue hoáº·c pull request.

## ğŸ“§ LiÃªn há»‡

Náº¿u cÃ³ cÃ¢u há»i hoáº·c váº¥n Ä‘á», vui lÃ²ng táº¡o issue trÃªn repository.

