# ğŸš€ Quick Start Guide

## CÃ i Ä‘áº·t nhanh (5 phÃºt)

### BÆ°á»›c 1: Install dependencies

```bash
cd /home/hoang/realtime-transcript/backend/pyanote
pip install -r requirements.txt
```

### BÆ°á»›c 2: Cháº¡y demo Ä‘áº§u tiÃªn

```bash
python test2.py
```

Báº¡n sáº½ tháº¥y output nhÆ°:

```
============================================================
VÃ Dá»¤ 1: Xá»­ lÃ½ audio chunk Ä‘áº§u tiÃªn
============================================================

ğŸ“Š Káº¿t quáº£ chunk 1:
  â±ï¸  6.6s â†’ 7.1s | ğŸ¤ SPEAKER_00
  â±ï¸  8.2s â†’ 9.5s | ğŸ¤ SPEAKER_01

ğŸ’¾ Speaker Memory: {
    'speakers': ['SPEAKER_00', 'SPEAKER_01'],
    'num_speakers': 2,
    'total_chunks': 1
}
```

## ğŸ¯ Use Case 1: Xá»­ lÃ½ audio chunks liÃªn tiáº¿p

```python
from test2 import RealtimeSpeakerDiarization
import torch

# Khá»Ÿi táº¡o pipeline
pipeline = RealtimeSpeakerDiarization(
    model_name="pyannote/speaker-diarization-community-1",
    token="YOUR_HF_TOKEN",  # Láº¥y tá»« https://huggingface.co/settings/tokens
    similarity_threshold=0.7,
    embedding_update_weight=0.3
)

pipeline.to(torch.device("cuda"))  # Hoáº·c "cpu" náº¿u khÃ´ng cÃ³ GPU

# Xá»­ lÃ½ chunk 1
output1 = pipeline("audio_chunk1.wav", use_memory=True)
for turn, _, speaker in output1.speaker_diarization.itertracks(yield_label=True):
    print(f"{speaker}: {turn.start:.1f}s - {turn.end:.1f}s")

# Xá»­ lÃ½ chunk 2 - speakers sáº½ consistent vá»›i chunk 1
output2 = pipeline("audio_chunk2.wav", use_memory=True)
for turn, _, speaker in output2.speaker_diarization.itertracks(yield_label=True):
    print(f"{speaker}: {turn.start:.1f}s - {turn.end:.1f}s")

# Xem thÃ´ng tin speakers
print(pipeline.get_speaker_info())
```

## ğŸ¯ Use Case 2: Xá»­ lÃ½ audio dÃ i

```python
from realtime_example import AudioStreamProcessor

processor = AudioStreamProcessor(hf_token="YOUR_HF_TOKEN")

# Tá»± Ä‘á»™ng chia thÃ nh chunks vÃ  xá»­ lÃ½
results = processor.process_long_audio(
    audio_path="long_audio.wav",
    chunk_duration=30.0,  # 30 giÃ¢y má»—i chunk
    overlap=1.0,          # 1 giÃ¢y overlap
    min_speakers=2,
    max_speakers=5
)

# Káº¿t quáº£ vá»›i timestamps Ä‘Ã£ Ä‘Æ°á»£c adjust
for result in results:
    print(f"Chunk {result['chunk_id']}: {result['speakers']}")
```

## ğŸ¯ Use Case 3: Realtime streaming

```python
import numpy as np
from realtime_example import AudioStreamProcessor

processor = AudioStreamProcessor(hf_token="YOUR_HF_TOKEN")

# Simulate realtime audio stream
for audio_chunk in your_audio_stream():  # Generator hoáº·c queue
    # audio_chunk: numpy array, shape (samples,)
    
    result = processor.process_audio_chunk(
        audio_chunk,
        sample_rate=16000,
        min_speakers=1,
        max_speakers=10
    )
    
    # Process results
    for segment in result['segments']:
        speaker = segment['speaker']
        start = segment['start']
        end = segment['end']
        
        # Gá»­i tá»›i UI, database, v.v.
        print(f"{speaker} spoke from {start}s to {end}s")

# Reset khi báº¯t Ä‘áº§u conversation má»›i
processor.reset()
```

## âš™ï¸ Tuning Parameters

### Khi nÃ o adjust `similarity_threshold`?

**Giáº£m threshold (0.6-0.7)** náº¿u:
- âŒ CÃ¹ng má»™t ngÆ°á»i bá»‹ chia thÃ nh nhiá»u speaker IDs
- âŒ Audio quality tháº¥p (noise, compression)
- âŒ Giá»ng nÃ³i thay Ä‘á»•i nhiá»u (cáº£m xÃºc, volume)

**TÄƒng threshold (0.8-0.9)** náº¿u:
- âŒ Nhiá»u ngÆ°á»i bá»‹ gá»™p thÃ nh má»™t speaker
- âŒ Giá»ng nÃ³i ráº¥t giá»‘ng nhau
- âŒ Audio quality cao, giá»ng á»•n Ä‘á»‹nh

### Khi nÃ o adjust `embedding_update_weight`?

**TÄƒng weight (0.4-0.6)** náº¿u:
- âŒ Giá»ng nÃ³i thay Ä‘á»•i nhanh (noise levels, distance to mic)
- âŒ Conversation dÃ i, giá»ng cÃ³ thá»ƒ thay Ä‘á»•i
- âŒ Cáº§n adapt nhanh vá»›i thay Ä‘á»•i

**Giáº£m weight (0.1-0.2)** náº¿u:
- âŒ Speaker IDs khÃ´ng á»•n Ä‘á»‹nh giá»¯a chunks
- âŒ Giá»ng nÃ³i á»•n Ä‘á»‹nh, khÃ´ng Ä‘á»•i nhiá»u
- âŒ Conversation ngáº¯n

## ğŸ”§ Common Configurations

### Video Conference (Zoom, Teams)
```python
pipeline = RealtimeSpeakerDiarization(
    similarity_threshold=0.75,      # Balanced
    embedding_update_weight=0.35    # Adapt to varying audio quality
)
```

### Phone Calls
```python
pipeline = RealtimeSpeakerDiarization(
    similarity_threshold=0.65,      # Relaxed (phone quality)
    embedding_update_weight=0.4     # Fast adapt to noise
)
```

### Podcast / High Quality Audio
```python
pipeline = RealtimeSpeakerDiarization(
    similarity_threshold=0.8,       # Strict (clear voices)
    embedding_update_weight=0.25    # Stable (consistent quality)
)
```

### Meeting Room Recording
```python
pipeline = RealtimeSpeakerDiarization(
    similarity_threshold=0.7,       # Balanced
    embedding_update_weight=0.3     # Standard
)
```

## ğŸ“Š Performance Tips

### 1. Use GPU
```python
import torch
pipeline.to(torch.device("cuda"))  # ~5-10x faster
```

### 2. Adjust batch sizes
```python
pipeline = RealtimeSpeakerDiarization(
    segmentation_batch_size=4,    # Larger = faster (needs more VRAM)
    embedding_batch_size=8
)
```

### 3. Optimal chunk duration
- **Too short** (< 5s): Poor embeddings, unstable
- **Too long** (> 60s): Slow processing, delays
- **Recommended**: 15-30 seconds

### 4. Use overlap
```python
processor.process_long_audio(
    chunk_duration=30.0,
    overlap=2.0  # 2s overlap helps continuity
)
```

## ğŸ› Troubleshooting

### Error: "CUDA out of memory"
```python
# Solution 1: Reduce batch size
pipeline.segmentation_batch_size = 1
pipeline.embedding_batch_size = 1

# Solution 2: Use CPU
pipeline.to(torch.device("cpu"))

# Solution 3: Shorter chunks
chunk_duration = 15.0  # instead of 30.0
```

### Warning: "Too many speakers detected"
```python
# Limit max speakers
output = pipeline(
    audio,
    min_speakers=2,
    max_speakers=5  # Reasonable upper bound
)
```

### Issue: "Inconsistent speaker IDs"
```python
# More stable configuration
pipeline = RealtimeSpeakerDiarization(
    similarity_threshold=0.75,      # Higher = more strict matching
    embedding_update_weight=0.2     # Lower = more stable
)
```

## ğŸ“š Next Steps

1. **Read full docs**: `README_REALTIME.md`
2. **See examples**: `realtime_example.py`
3. **WebSocket integration**: `websocket_server.py`
4. **Technical details**: `SOLUTION_OVERVIEW.md`

## ğŸ’¡ Pro Tips

### Tip 1: Monitor speaker info
```python
info = pipeline.get_speaker_info()
if info['num_speakers'] > expected_speakers:
    # Adjust similarity_threshold
    pipeline.similarity_threshold = 0.8
```

### Tip 2: Reset giá»¯a conversations
```python
# Between different calls/meetings
pipeline.reset_context()
```

### Tip 3: Save/load speaker memory
```python
import pickle

# Save
memory = {
    'speaker_memory': pipeline.speaker_memory,
    'speaker_counts': pipeline.speaker_counts,
}
with open('speakers.pkl', 'wb') as f:
    pickle.dump(memory, f)

# Load
with open('speakers.pkl', 'rb') as f:
    memory = pickle.load(f)
    pipeline.speaker_memory = memory['speaker_memory']
    pipeline.speaker_counts = memory['speaker_counts']
```

### Tip 4: Logging
```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Log má»—i chunk
result = pipeline(audio)
logger.info(f"Processed chunk with {len(result.speakers)} speakers")
logger.info(f"Total known speakers: {pipeline.get_speaker_info()['num_speakers']}")
```

## ğŸ‰ You're Ready!

BÃ¢y giá» báº¡n cÃ³ thá»ƒ:
- âœ… Xá»­ lÃ½ audio chunks vá»›i persistent speaker IDs
- âœ… Track speakers xuyÃªn suá»‘t conversation
- âœ… Tune parameters cho use case cá»§a báº¡n
- âœ… Integrate vÃ o production system

**Happy diarizing! ğŸ¤âœ¨**

