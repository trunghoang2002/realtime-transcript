## üß© 1. T·ªïng quan v·ªÅ `SenseVoice`

`SenseVoice` l√† m√¥ h√¨nh **ƒëa nhi·ªám (multi-task)** trong h·ªá sinh th√°i **FunAudioLLM**, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ kh√¥ng ch·ªâ nh·∫≠n d·∫°ng ti·∫øng n√≥i (ASR) m√† c√≤n **hi·ªÉu gi·ªçng n√≥i** ‚Äî bao g·ªìm:

* **ASR** ‚Äì Automatic Speech Recognition (chuy·ªÉn l·ªùi n√≥i th√†nh vƒÉn b·∫£n)
* **LID** ‚Äì Language Identification (ph√°t hi·ªán ng√¥n ng·ªØ)
* **SER** ‚Äì Speech Emotion Recognition (nh·∫≠n di·ªán c·∫£m x√∫c)
* **AED** ‚Äì Audio Event Detection (ph√°t hi·ªán s·ª± ki·ªán √¢m thanh: v·ªó tay, c∆∞·ªùi, ho, nh·∫°c, v.v.)

Phi√™n b·∫£n ph·ªï bi·∫øn nh·∫•t l√† `SenseVoiceSmall`, c√≥ ∆∞u ƒëi·ªÉm **ƒë·ªô tr·ªÖ r·∫•t th·∫•p (~70 ms cho 10 s audio)**, h·ªó tr·ª£ h∆°n **50 ng√¥n ng·ªØ**, ho·∫°t ƒë·ªông t·ªët c·∫£ tr√™n **CPU** v√† **GPU**.

```python
from funasr import AutoModel

model = AutoModel(
    model="FunAudioLLM/SenseVoiceSmall",
    vad_model="fsmn-vad",                    # T√≠ch h·ª£p VAD (ph√°t hi·ªán gi·ªçng n√≥i)
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda"                          # ho·∫∑c "cpu"
)
```

---

## ‚öôÔ∏è 2. C√°c tham s·ªë ch√≠nh khi kh·ªüi t·∫°o

| Tham s·ªë               | M√¥ t·∫£ chi ti·∫øt                                                                                         |
| --------------------- | ------------------------------------------------------------------------------------------------------ |
| **model**             | T√™n ho·∫∑c ƒë∆∞·ªùng d·∫´n model. <br>V√≠ d·ª•: `"FunAudioLLM/SenseVoiceSmall"` ho·∫∑c model c·ª•c b·ªô.                |
| **vad_model**         | B·∫≠t/t·∫Øt VAD (Voice Activity Detection). <br>`"fsmn-vad"` ƒë·ªÉ b·∫≠t ho·∫∑c `None` ƒë·ªÉ t·∫Øt.                    |
| **vad_kwargs**        | Dict c·∫•u h√¨nh cho VAD, v√≠ d·ª•:<br>`{"max_single_segment_time": 30000}` (ms) ‚Äì th·ªùi l∆∞·ª£ng t·ªëi ƒëa 1 ƒëo·∫°n. |
| **device**            | `"cuda:0"` / `"cpu"`. N√™n d√πng GPU ƒë·ªÉ ƒë·∫°t t·ªëc ƒë·ªô realtime.                                             |
| **hub**               | (Tu·ª≥ ch·ªçn) Ngu·ªìn t·∫£i model, `"hf"` (HuggingFace) ho·∫∑c `"ms"` (ModelScope).                             |
| **trust_remote_code** | `True` n·∫øu mu·ªën cho ph√©p model t·∫£i code tu·ª≥ ch·ªânh t·ª´ repo g·ªëc.                                         |

---

## üîç 3. Ph∆∞∆°ng th·ª©c `generate()`

Ph∆∞∆°ng th·ª©c ch√≠nh c·ªßa SenseVoice ƒë·ªÉ x·ª≠ l√Ω audio.

```python
results = model.generate(
    input="sample.wav",          # Ho·∫∑c numpy.ndarray float32
    cache={},                    # D√πng khi streaming
    language="auto",             # "vi", "en", "ja", "auto"...
    use_itn=False,               # Kh√¥ng chuy·ªÉn ƒë·ªïi s·ªë ‚Üí ch·ªØ
    batch_size_s=10,             # S·ªë gi√¢y audio m·ªói batch
    merge_vad=True,              # Gom ƒëo·∫°n sau khi VAD
    merge_length_s=5,            # Gi·ªõi h·∫°n merge
)
```

### üìò Gi·∫£i th√≠ch c√°c tham s·ªë quan tr·ªçng

| Nh√≥m                   | Tham s·ªë                              | Gi·∫£i th√≠ch                                                                 |
| ---------------------- | ------------------------------------ | -------------------------------------------------------------------------- |
| **Input**              | `input`                              | ƒê∆∞·ªùng d·∫´n file audio ho·∫∑c m·∫£ng `numpy.float32` mono 16kHz.                 |
| **Ng√¥n ng·ªØ & VAD**     | `language`, `vad_model`, `merge_vad` | `language="auto"` cho t·ª± ƒë·ªông, `merge_vad=True` ƒë·ªÉ gh√©p c√°c ƒëo·∫°n g·∫ßn nhau. |
| **Batch & hi·ªáu nƒÉng**  | `batch_size_s`, `merge_length_s`     | ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc batch (gi√¢y). Realtime n√™n d√πng 5‚Äì10 s.              |
| **Text normalization** | `use_itn`                            | `True`: ƒë·ªïi ‚Äú123‚Äù ‚Üí ‚Äúm·ªôt trƒÉm hai m∆∞∆°i ba‚Äù, `False`: gi·ªØ nguy√™n s·ªë.        |
| **Streaming**          | `cache`                              | Duy tr√¨ ng·ªØ c·∫£nh khi x·ª≠ l√Ω li√™n t·ª•c qua c√°c chunk audio.                   |
| **Output detail**      | `rich` (n·ªôi b·ªô)                      | M·ªôt s·ªë phi√™n b·∫£n c√≥ th·ªÉ tr·∫£ th√™m `emotion`, `event`, `lang`.               |

---

## üß† 4. Output tr·∫£ v·ªÅ

H√†m `generate()` tr·∫£ v·ªÅ **list dict**, m·ªói ph·∫ßn t·ª≠ m√¥ t·∫£ m·ªôt ƒëo·∫°n audio.

```python
[
  {
    "text": "Xin ch√†o, t√¥i l√† tr·ª£ l√Ω ·∫£o.",
    "key": "rand_key_gUe52RvEJgwBu"
  },
  ...
]
```

M·ªói ph·∫ßn t·ª≠ c√≥:

* `text` ‚Äî n·ªôi dung nh·∫≠n d·∫°ng
* `key`  ‚Äî key

---

## ‚ö° 5. C√°c ch·∫ø ƒë·ªô t·ªëi ∆∞u hi·ªáu nƒÉng

| Tr∆∞·ªùng h·ª£p                   | C·∫•u h√¨nh g·ª£i √Ω                                                                 |
| ---------------------------- | ------------------------------------------------------------------------------ |
| **Realtime, latency th·∫•p**   | `batch_size_s=5`, `vad_model="fsmn-vad"`, `merge_vad=False`, `language="auto"` |
| **Ch√≠nh x√°c cao (file d√†i)** | `batch_size_s=60`, `merge_vad=True`, `merge_length_s=10`, `language="vi"`      |
| **CPU-only**                 | `device="cpu"`, `batch_size_s=10`, `vad_model=None` ƒë·ªÉ b·ªè ph√¢n ƒëo·∫°n t·ª± ƒë·ªông.   |
| **GPU m·∫°nh (RTX/V100)**      | `device="cuda:0"`, `batch_size_s=20`, `merge_vad=True`.                        |
| **Streaming WebSocket**      | S·ª≠ d·ª•ng `cache={}` v√† chunk audio 1‚Äì2 s ƒë·ªÉ inference n·ªëi ti·∫øp.                 |

---

## üß© 6. So s√°nh nhanh c√°c model SenseVoice

| Model             | K√≠ch th∆∞·ªõc | Ng√¥n ng·ªØ | RAM c·∫ßn (FP16) | Latency (10 s audio) | T√°c v·ª• h·ªó tr·ª£      |
| ----------------- | ---------- | -------- | -------------- | -------------------- | ------------------ |
| `SenseVoiceSmall` | ~150 MB    | > 50     | ~1 GB          | ~70 ms               | ASR, LID, SER, AED |
| `SenseVoiceBase`  | ~350 MB    | > 50     | ~2 GB          | ~120 ms              | ASR, LID, SER, AED |
| `SenseVoiceLarge` | ~800 MB    | > 50     | ~5 GB          | ~250 ms              | ASR, LID, SER, AED |

---

## üß™ 7. V√≠ d·ª• ƒë·∫ßy ƒë·ªß

```python
from funasr import AutoModel
import soundfile as sf

model = AutoModel(
    model="FunAudioLLM/SenseVoiceSmall",
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 15000},
    device="cuda:0"
)

audio, sr = sf.read("sample.wav")
assert sr == 16000

res = model.generate(
    input=audio,
    language="vi",
    use_itn=False,
    batch_size_s=10,
    merge_vad=True
)

for r in res:
    print(f"{r['text']}")
```

---

## ‚úÖ 8. Khi d√πng cho realtime transcript

| M·ª•c ti√™u              | Khuy·∫øn ngh·ªã                                                              |
| --------------------- | ------------------------------------------------------------------------ |
| T·ªëc ƒë·ªô ph·∫£n h·ªìi nhanh | D√πng `SenseVoiceSmall`, b·∫≠t `vad_model`, `batch_size_s=5`                |
| Gi·∫£m tr·ªÖ              | G·ª≠i chunk audio 1 s qua WebSocket v√† g·ªçi `model.generate()` li√™n t·ª•c     |
| Ti·∫øng Vi·ªát ·ªïn ƒë·ªãnh    | √âp `language="vi"` ƒë·ªÉ tr√°nh nh·∫≠n sai                                     |
| Ch·ªëng nhi·ªÖu mic       | B·∫≠t `fsmn-vad` v√† gi·∫£m `max_single_segment_time` xu·ªëng 10 s              |
| K·∫øt h·ª£p emotion/event | B·∫≠t xu·∫•t `emotion`, `event` ƒë·ªÉ th√™m c·∫£m x√∫c ho·∫∑c nh·∫°c n·ªÅn v√†o transcript |

---