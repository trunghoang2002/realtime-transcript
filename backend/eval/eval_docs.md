# üìä Evaluation Scripts Documentation

H·ªá th·ªëng ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng cho 2 t√°c v·ª• ch√≠nh:
1. **ASR Evaluation** (`eval_asr.py`): ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng nh·∫≠n d·∫°ng gi·ªçng n√≥i (Speech Recognition)
2. **Diarization Evaluation** (`eval_diarization.py`): ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ph√¢n bi·ªát ng∆∞·ªùi n√≥i (Speaker Verification)

---

# üé§ Part 1: ASR Evaluation

## üìã M·ª•c ƒë√≠ch

Script `eval_asr.py` ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ:
- ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√°c model ASR (Whisper, SenseVoice, Gemini, VLLM)
- ƒêo l∆∞·ªùng hi·ªáu su·∫•t x·ª≠ l√Ω th·ªùi gian th·ª±c (Real-Time Factor - RTF)
- So s√°nh hi·ªáu su·∫•t gi·ªØa c√°c model kh√°c nhau
- ƒê√°nh gi√° v·ªõi/kh√¥ng c√≥ VAD filtering
- H·ªó tr·ª£ checkpoint ƒë·ªÉ ti·∫øp t·ª•c evaluation khi b·ªã gi√°n ƒëo·∫°n

## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng

### Base Class: `BaseASR`
Abstract class ƒë·ªãnh nghƒ©a interface chung cho t·∫•t c·∫£ ASR models:
- `_transcribe_no_vad(audio)`: Transcribe kh√¥ng d√πng VAD
- `_transcribe_with_vad(audio)`: Transcribe c√≥ d√πng VAD
- `transcribe(file_path, vad_filter)`: Public API

### Supported ASR Models

#### 1. **WhisperJA** (Faster Whisper)
```python
transcriber = WhisperJA(
    model_name="large-v3",  # tiny, base, small, medium, large, large-v1, large-v2, large-v3, turbo
    device="cuda",
    compute_type="float16",
    beam_size=5,
    best_of=5,
    temperature=0.0
)
```

**Features:**
- H·ªó tr·ª£ multiple model sizes
- T·ªëi ∆∞u v·ªõi faster-whisper (CTranslate2)
- VAD filtering v·ªõi Silero VAD
- Beam search v√† temperature control

#### 2. **SenseVoiceJA** (FunASR)
```python
transcriber = SenseVoiceJA(
    model_name="iic/SenseVoiceSmall",
    device="cuda",
    max_single_segment_time=30000,
    batch_size_s=20,
    use_itn=False
)
```

**Features:**
- Optimized cho ti·∫øng Nh·∫≠t
- Batch processing
- Inverse Text Normalization (ITN)

#### 3. **GeminiASR** (Google Gemini)
```python
transcriber = GeminiASR(
    api_key=os.getenv("GEMINI_API_KEY"),
    model_name="gemini-2.5-pro"  # gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro
)
```

**Features:**
- Cloud-based ASR
- H·ªó tr·ª£ multiple languages
- Automatic retry v·ªõi exponential backoff
- Temperature=0.0 cho reproducibility

#### 4. **VllmASR** (vLLM Server)
```python
transcriber = VllmASR(
    base_url="https://your-vllm-server.com/v1",
    api_key="EMPTY",
    model_name="Qwen/Qwen2-Audio-7B-Instruct",
    prompt="Transcribe the audio into text."
)
```

**Features:**
- OpenAI-compatible API
- Custom prompt engineering
- Base64 audio encoding
- Exponential backoff retry

## üìä Metrics

### 1. WER (Word Error Rate)
- **ƒê·ªãnh nghƒ©a**: T·ª∑ l·ªá l·ªói t·ª´
- **C√¥ng th·ª©c**: `WER = (S + D + I) / N`
  - S: S·ªë t·ª´ b·ªã thay th·∫ø (Substitutions)
  - D: S·ªë t·ª´ b·ªã x√≥a (Deletions)
  - I: S·ªë t·ª´ b·ªã th√™m v√†o (Insertions)
  - N: T·ªïng s·ªë t·ª´ trong ground truth
- **Gi√° tr·ªã**: 0.0 = ho√†n h·∫£o, c√†ng cao c√†ng k√©m
- **Tokenization**: S·ª≠ d·ª•ng Sudachi tokenizer cho ti·∫øng Nh·∫≠t

### 2. CER (Character Error Rate)
- **ƒê·ªãnh nghƒ©a**: T·ª∑ l·ªá l·ªói k√Ω t·ª±
- **C√¥ng th·ª©c**: `CER = (S + D + I) / N` (t√≠nh theo k√Ω t·ª±)
- **Gi√° tr·ªã**: 0.0 = ho√†n h·∫£o, c√†ng cao c√†ng k√©m
- **H·ªØu √≠ch cho**: Ng√¥n ng·ªØ kh√¥ng c√≥ kho·∫£ng tr·∫Øng (ti·∫øng Nh·∫≠t, ti·∫øng Trung)

### 3. RTF (Real-Time Factor)
- **ƒê·ªãnh nghƒ©a**: T·ª∑ l·ªá gi·ªØa th·ªùi gian x·ª≠ l√Ω v√† ƒë·ªô d√†i audio
- **C√¥ng th·ª©c**: `RTF = processing_time / audio_duration`
- **Gi√° tr·ªã**: 
  - RTF < 1.0: X·ª≠ l√Ω nhanh h∆°n realtime (t·ªët)
  - RTF = 1.0: X·ª≠ l√Ω ƒë√∫ng b·∫±ng realtime
  - RTF > 1.0: X·ª≠ l√Ω ch·∫≠m h∆°n realtime (kh√¥ng ƒë·ªß cho realtime)

## üîÑ Text Normalization

Tr∆∞·ªõc khi t√≠nh WER/CER, text ƒë∆∞·ª£c normalize:

```python
def eval_score(ground_truth, prediction):
    # 1. Lowercase
    ground_truth = ground_truth.lower()
    prediction = prediction.lower()
    
    # 2. Remove punctuation (Japanese)
    pattern = r"[\p{P}ÔΩû~ÔºãÔºùÔºÑ|]+"
    ground_truth = re.sub(pattern, "", ground_truth)
    prediction = re.sub(pattern, "", prediction)
    
    # 3. Remove tags
    prediction = prediction.replace("<[^>]*>", "")
    
    # 4. Remove English/spaces
    ground_truth = re.sub(r"[A-Za-z\s]+", "", ground_truth).strip()
    prediction = re.sub(r"[A-Za-z\s]+", "", prediction).strip()
    
    # 5. Calculate metrics with Japanese tokenizer
    wer_score = wer(ground_truth, prediction, 
                    reference_transform=wer_ja, 
                    hypothesis_transform=wer_ja)
    cer_score = cer(ground_truth, prediction)
    return wer_score, cer_score
```

## üíæ Checkpoint Mechanism

Script h·ªó tr·ª£ checkpoint ƒë·ªÉ ti·∫øp t·ª•c evaluation khi b·ªã gi√°n ƒëo·∫°n:

```python
results_file = "eval_results_checkpoint.csv"
```

**Features:**
- T·ª± ƒë·ªông l∆∞u k·∫øt qu·∫£ sau m·ªói test case
- Resume t·ª´ file checkpoint n·∫øu script b·ªã d·ª´ng
- Track completed files ƒë·ªÉ kh√¥ng x·ª≠ l√Ω l·∫°i

**CSV Format:**
```csv
file_path,ground_truth,prediction,wer_score,cer_score,rtf,audio_duration,processing_time
```

## üöÄ C√°ch s·ª≠ d·ª•ng

### 1. Chu·∫©n b·ªã dataset
Dataset CSV v·ªõi format:
```csv
...,file_path,ground_truth
...,audio/file1.wav,„Åì„Çå„ÅØ„ÉÜ„Çπ„Éà„Åß„Åô
...,audio/file2.wav,Èü≥Â£∞Ë™çË≠ò„ÅÆË©ï‰æ°
```

### 2. C·∫•u h√¨nh transcriber
Uncomment model b·∫°n mu·ªën test trong `eval_asr.py`:

```python
# Option 1: Whisper
transcriber = WhisperJA(
    model_name="large-v1",
    device="cuda",
    compute_type="float16"
)

# Option 2: SenseVoice
# transcriber = SenseVoiceJA(...)

# Option 3: Gemini
# transcriber = GeminiASR(...)

# Option 4: VLLM
# transcriber = VllmASR(...)
```

### 3. Ch·∫°y evaluation
```bash
cd backend/eval
python eval_asr.py
```

### 4. Xem k·∫øt qu·∫£
```
Total test cases: 400
Remaining: 350
Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400
==================================================
FINAL RESULTS
==================================================
Total evaluated: 400 test cases
WER: 0.1234
CER: 0.0567
RTF: 0.4567 (mean)
RTF: 0.4123 (median)
RTF: 0.2100 (min) | 0.9800 (max)
```

---

# üë• Part 2: Diarization Evaluation

## üìã M·ª•c ƒë√≠ch

Script `eval_diarization.py` ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ:
- ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng speaker verification/diarization
- So s√°nh multiple embeddings t·ª´ nhi·ªÅu models: Pyannote, SpeechBrain, NeMo (TitaNet, ECAPA-TDNN)
- H·ªó tr·ª£ multiple instances c·ªßa c√πng model type (v√≠ d·ª•: 2 NeMo models v·ªõi configs kh√°c nhau)
- T√≠nh to√°n multiple metrics: EER, FAR, FRR, Precision, Recall, F1, AUC
- Visualize ROC curves, DET curves, Precision-Recall curves
- T√¨m optimal threshold cho t·ª´ng metric

## üîß Dependencies

```bash
pip install scikit-learn
pip install matplotlib
pip install tqdm
pip install numpy
```

## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng

### Pipeline
```python
from fusion_diarization import RealtimeSpeakerDiarization

# Fusion pipeline v·ªõi multiple model instances
pipeline = RealtimeSpeakerDiarization(
    models=[
        ('nemo', 'nemo_titanet'),
        ('nemo', 'nemo_ecapa_tdnn'),
        ('pyannote', 'pyan_community'),
        ('speechbrain', 'sb_default')
    ],
    model_configs={
        'nemo_titanet': {'pretrained_speaker_model': 'titanet_large'},
        'nemo_ecapa_tdnn': {'pretrained_speaker_model': 'ecapa_tdnn'},
        'pyan_community': {
            'model_name': "pyannote/speaker-diarization-community-1",
            'token': os.getenv("HF_TOKEN")
        },
        'sb_default': {}
    }
)
```

Pipeline extract 4 lo·∫°i embeddings t·ª´ multiple model instances:
- **NeMo TitaNet embeddings**: From NVIDIA NeMo TitaNet Large
- **NeMo ECAPA-TDNN embeddings**: From NVIDIA NeMo ECAPA-TDNN
- **Pyannote embeddings**: From pyannote.audio
- **SpeechBrain embeddings**: From SpeechBrain ECAPA-TDNN

**L∆∞u √Ω**: B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng nhi·ªÅu instances c·ªßa c√πng model type v·ªõi configs kh√°c nhau. V√≠ d·ª•: 2 NeMo models v·ªõi pretrained models kh√°c nhau.

### Evaluation Process

1. **List speakers and utterances**: Scan dataset folder
2. **Build trials**: T·∫°o genuine pairs (c√πng speaker) v√† impostor pairs (kh√°c speaker)
3. **Extract embeddings**: Extract 1 l·∫ßn cho t·∫•t c·∫£ files
4. **Compute scores**: T√≠nh cosine similarity cho t·ª´ng trial
5. **Calculate metrics**: EER, FAR, FRR, Precision, Recall, F1, AUC
6. **Visualize**: V·∫Ω v√† l∆∞u ROC, DET, PR curves

## üìä Metrics

### 1. EER (Equal Error Rate)
- **ƒê·ªãnh nghƒ©a**: ƒêi·ªÉm m√† FAR = FRR
- **Gi√° tr·ªã**: C√†ng th·∫•p c√†ng t·ªët (0% = ho√†n h·∫£o)
- **√ù nghƒ©a**: C√¢n b·∫±ng gi·ªØa false accept v√† false reject

### 2. FAR (False Acceptance Rate)
- **ƒê·ªãnh nghƒ©a**: T·ª∑ l·ªá impostor pairs b·ªã accept nh·∫ßm
- **C√¥ng th·ª©c**: `FAR = FP / (FP + TN)`
- **Gi√° tr·ªã**: C√†ng th·∫•p c√†ng t·ªët

### 3. FRR (False Rejection Rate)
- **ƒê·ªãnh nghƒ©a**: T·ª∑ l·ªá genuine pairs b·ªã reject nh·∫ßm
- **C√¥ng th·ª©c**: `FRR = FN / (FN + TP)`
- **Gi√° tr·ªã**: C√†ng th·∫•p c√†ng t·ªët

### 4. Precision
- **C√¥ng th·ª©c**: `Precision = TP / (TP + FP)`
- **Gi√° tr·ªã**: 0.0 - 1.0 (c√†ng cao c√†ng t·ªët)

### 5. Recall (Sensitivity, TPR)
- **C√¥ng th·ª©c**: `Recall = TP / (TP + FN)`
- **Gi√° tr·ªã**: 0.0 - 1.0 (c√†ng cao c√†ng t·ªët)

### 6. F1 Score
- **C√¥ng th·ª©c**: `F1 = 2 * (Precision * Recall) / (Precision + Recall)`
- **Gi√° tr·ªã**: 0.0 - 1.0 (c√†ng cao c√†ng t·ªët)

### 7. AUC (Area Under ROC Curve)
- **ƒê·ªãnh nghƒ©a**: Di·ªán t√≠ch d∆∞·ªõi ROC curve
- **Gi√° tr·ªã**: 0.0 - 1.0 (1.0 = perfect classifier)

## üìÅ Dataset Structure

Dataset c·∫ßn tu√¢n theo c·∫•u tr√∫c:
```
dataset/
‚îú‚îÄ‚îÄ speaker_001/
‚îÇ   ‚îú‚îÄ‚îÄ falset10/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wav24kHz16bit/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ file1.wav
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ file2.wav
‚îÇ   ‚îú‚îÄ‚îÄ nonpara30/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wav24kHz16bit/
‚îÇ   ‚îú‚îÄ‚îÄ parallel100/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wav24kHz16bit/
‚îÇ   ‚îî‚îÄ‚îÄ whisper10/
‚îÇ       ‚îî‚îÄ‚îÄ wav24kHz16bit/
‚îú‚îÄ‚îÄ speaker_002/
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

**Y√™u c·∫ßu:**
- M·ªói speaker c√≥ √≠t nh·∫•t 2 utterances
- 4 lo·∫°i folder: `falset10`, `nonpara30`, `parallel100`, `whisper10`
- Audio files trong `wav24kHz16bit/`

## üéØ Trial Generation

### Genuine Pairs (label=1)
- C·∫∑p 2 utterances kh√°c nhau c·ªßa c√πng 1 speaker
- Max pairs per speaker: `max_genuine_per_spk=50`

### Impostor Pairs (label=0)
- C·∫∑p utterances t·ª´ 2 speakers kh√°c nhau
- Pairs per speaker: `impostor_per_spk=100`

### Example
```python
trials = build_trials(
    spk2utts,
    max_genuine_per_spk=50,  # Max 50 genuine pairs m·ªói speaker
    impostor_per_spk=100     # 100 impostor pairs m·ªói speaker
)
# Output: [(path1, path2, label), ...]
```

## üé® Visualization

Script t·ª± ƒë·ªông t·∫°o 3 lo·∫°i bi·ªÉu ƒë·ªì trong folder `eval_results/`:

### 1. ROC Curve (`roc_curves.png`)
- **Tr·ª•c X**: False Positive Rate (FAR)
- **Tr·ª•c Y**: True Positive Rate (1 - FRR)
- **Features**:
  - So s√°nh t·∫•t c·∫£ models (Pyannote, SpeechBrain, NeMo variants)
  - Hi·ªÉn th·ªã AUC score cho m·ªói model
  - ƒê√°nh d·∫•u ƒëi·ªÉm EER cho m·ªói curve
  - ƒê∆∞·ªùng baseline (random classifier)
- **Color coding**:
  - Blue: Pyannote
  - Red: SpeechBrain  
  - Green: NeMo TitaNet
  - Yellow: NeMo ECAPA-TDNN

### 2. DET Curve (`det_curves.png`)
- **Tr·ª•c X**: False Acceptance Rate (%)
- **Tr·ª•c Y**: False Rejection Rate (%)
- **Features**:
  - D·ªÖ nh√¨n h∆°n cho speaker verification
  - ƒê√°nh d·∫•u ƒëi·ªÉm EER cho m·ªói model
  - ƒê∆∞·ªùng ch√©o FAR=FRR (EER line)
- **Color coding**: Gi·ªëng ROC curves
  - Blue: Pyannote | Red: SpeechBrain
  - Green: NeMo TitaNet | Yellow: NeMo ECAPA-TDNN

### 3. Precision-Recall Curve (`precision_recall_curves.png`)
- **Tr·ª•c X**: Recall
- **Tr·ª•c Y**: Precision
- **Features**:
  - Hi·ªÉn th·ªã PR AUC cho m·ªói model
  - ƒê√°nh d·∫•u ƒëi·ªÉm Best F1 cho m·ªói curve
  - So s√°nh t·∫•t c·∫£ embeddings
- **Color coding**: Gi·ªëng ROC/DET curves
  - Blue: Pyannote | Red: SpeechBrain
  - Green: NeMo TitaNet | Yellow: NeMo ECAPA-TDNN

## üéØ Multiple Model Instances

Script h·ªó tr·ª£ ƒë√°nh gi√° multiple instances c·ªßa c√πng model type v·ªõi configs kh√°c nhau:

### V√≠ d·ª•: Multiple NeMo Models
```python
pipeline = RealtimeSpeakerDiarization(
    models=[
        ('nemo', 'nemo_titanet'),        # NeMo v·ªõi TitaNet Large
        ('nemo', 'nemo_ecapa_tdnn'),     # NeMo v·ªõi ECAPA-TDNN
        ('pyannote', 'pyan_community'),   # Pyannote
        ('speechbrain', 'sb_default')     # SpeechBrain
    ],
    model_configs={
        'nemo_titanet': {'pretrained_speaker_model': 'titanet_large'},
        'nemo_ecapa_tdnn': {'pretrained_speaker_model': 'ecapa_tdnn'},
        'pyan_community': {
            'model_name': "pyannote/speaker-diarization-community-1",
            'token': os.getenv("HF_TOKEN")
        },
        'sb_default': {}
    }
)
```

### L·ª£i √≠ch:
- **So s√°nh variants**: So s√°nh hi·ªáu su·∫•t gi·ªØa c√°c pretrained models kh√°c nhau
- **T·ªëi ∆∞u selection**: Ch·ªçn model config t·ªët nh·∫•t cho dataset c·ª• th·ªÉ
- **Ensemble insights**: Hi·ªÉu ƒë∆∞·ª£c ƒë√≥ng g√≥p c·ªßa t·ª´ng model trong fusion
- **Efficient extraction**: Extract t·∫•t c·∫£ embeddings ch·ªâ 1 l·∫ßn

### Caching System:
Script c√≥ caching mechanism ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô:
- **Auto cache**: T·ª± ƒë·ªông l∆∞u embeddings sau khi extract
- **Cache key**: D·ª±a tr√™n MD5 hash c·ªßa danh s√°ch files
- **Reusable**: Cache c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng cho nhi·ªÅu l·∫ßn ch·∫°y
- **Clear cache**: S·ª≠ d·ª•ng `clear_cache()` ƒë·ªÉ x√≥a cache c≈©

```python
# S·ª≠ d·ª•ng cache (m·∫∑c ƒë·ªãnh)
results = evaluate_dataset("dataset/jvs_ver1", use_cache=True)

# Force re-extract (kh√¥ng d√πng cache)
results = evaluate_dataset("dataset/jvs_ver1", use_cache=False)

# X√≥a t·∫•t c·∫£ cache
clear_cache()
```

## üöÄ C√°ch s·ª≠ d·ª•ng

### 1. Chu·∫©n b·ªã dataset
Organize audio files theo c·∫•u tr√∫c folder nh∆∞ tr√™n.

### 2. C·∫•u h√¨nh pipeline
Trong `eval_diarization.py`, c·∫•u h√¨nh fusion pipeline:
```python
pipeline = RealtimeSpeakerDiarization(
    models=[
        ('nemo', 'nemo_titanet'),
        ('nemo', 'nemo_ecapa_tdnn'),
        ('pyannote', 'pyan_community'),
        ('speechbrain', 'sb_default')
    ],
    model_configs={
        'nemo_titanet': {'pretrained_speaker_model': 'titanet_large'},
        'nemo_ecapa_tdnn': {'pretrained_speaker_model': 'ecapa_tdnn'},
        'pyan_community': {
            'model_name': "pyannote/speaker-diarization-community-1",
            'token': os.getenv("HF_TOKEN")
        },
        'sb_default': {}
    }
)
```

### 3. C·∫•u h√¨nh evaluation (optional)
C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh c√°c tham s·ªë:
```python
# S·ªë l∆∞·ª£ng trials
max_genuine_per_spk = 50
impostor_per_spk = 100

# Output directory cho bi·ªÉu ƒë·ªì
output_dir = "eval_results"

# Dataset path
dataset_path = "dataset/jvs_ver1"

# Cache settings
use_cache = True  # False ƒë·ªÉ force re-extract
```

### 4. Ch·∫°y evaluation
```bash
cd backend/eval
python eval_diarization.py
```

### 5. Xem k·∫øt qu·∫£

**Console Output:**
```
Found 100 speakers usable.
Total trials: 15000
Extracting embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500

=== Evaluating pyannote embeddings ===
Computing metrics on 14850 valid trials
EER: 5.23% | FAR@EER: 5.21% | FRR@EER: 5.25% | Thr(EER): 0.6234
Precision@EER: 94.77% | Recall@EER: 94.75% | F1@EER: 94.76%
Best F1: 96.12% | Precision@F1: 95.89% | Recall@F1: 96.35% | Thr(F1): 0.6789
AUC: 0.9876

=== Evaluating speechbrain embeddings ===
Computing metrics on 14850 valid trials
EER: 4.87% | FAR@EER: 4.85% | FRR@EER: 4.89% | Thr(EER): 0.7123
Precision@EER: 95.13% | Recall@EER: 95.11% | F1@EER: 95.12%
Best F1: 96.87% | Precision@F1: 96.45% | Recall@F1: 97.29% | Thr(F1): 0.7456
AUC: 0.9912

=== Evaluating nemo_titanet embeddings ===
Computing metrics on 14850 valid trials
EER: 3.45% | FAR@EER: 3.42% | FRR@EER: 3.48% | Thr(EER): 0.7456
Precision@EER: 96.55% | Recall@EER: 96.52% | F1@EER: 96.53%
Best F1: 97.89% | Precision@F1: 97.65% | Recall@F1: 98.13% | Thr(F1): 0.7890
AUC: 0.9945

=== Evaluating nemo_ecapa_tdnn embeddings ===
Computing metrics on 14850 valid trials
EER: 4.12% | FAR@EER: 4.10% | FRR@EER: 4.14% | Thr(EER): 0.7234
Precision@EER: 95.88% | Recall@EER: 95.86% | F1@EER: 95.87%
Best F1: 97.34% | Precision@F1: 97.12% | Recall@F1: 97.56% | Thr(F1): 0.7567
AUC: 0.9928

=== Plotting curves ===
ROC curve saved to: eval_results/roc_curves.png
DET curve saved to: eval_results/det_curves.png
Precision-Recall curve saved to: eval_results/precision_recall_curves.png
```

**Generated Files:**
- `eval_results/roc_curves.png` - So s√°nh ROC curves c·ªßa t·∫•t c·∫£ models
- `eval_results/det_curves.png` - So s√°nh DET curves c·ªßa t·∫•t c·∫£ models  
- `eval_results/precision_recall_curves.png` - So s√°nh PR curves c·ªßa t·∫•t c·∫£ models
- `eval_cache/embeddings_cache_*.pkl` - Cached embeddings (auto-generated)

**Final Results Output:**
```python
=== Final Results ===
Pyannote: {'EER': 0.0523, 'AUC': 0.9876, ...}
SpeechBrain: {'EER': 0.0487, 'AUC': 0.9912, ...}
NeMo Titanet: {'EER': 0.0345, 'AUC': 0.9945, ...}
NeMo Ecapa TDNN: {'EER': 0.0412, 'AUC': 0.9928, ...}
```

## üõ°Ô∏è Error Handling

Script x·ª≠ l√Ω robust v·ªõi c√°c edge cases:

### 1. Embedding Extraction Failures
```python
# Skip n·∫øu:
- result is None
- embeddings is None or empty
- embeddings to√†n b·∫±ng 0
- embeddings to√†n l√† NaN
```

### 2. Score Computation Failures
```python
# Skip trial n·∫øu:
- File kh√¥ng c√≥ trong cache
- Embedding type kh√¥ng t·ªìn t·∫°i
- Embedding l√† None
- Embedding to√†n 0 ho·∫∑c NaN
- Cosine score l√† NaN ho·∫∑c inf
```

### 3. Metrics Calculation
```python
# Return None n·∫øu:
- Kh√¥ng c√≥ valid trials
- zero_division=0 trong precision_recall_fscore_support
```

## üîç Gi·∫£i th√≠ch k·∫øt qu·∫£

### EER th·∫•p (< 5%)
- ‚úÖ Ch·∫•t l∆∞·ª£ng speaker verification xu·∫•t s·∫Øc
- System ph√¢n bi·ªát speakers r·∫•t ch√≠nh x√°c

### EER trung b√¨nh (5% - 10%)
- ‚ö†Ô∏è Ch·∫•t l∆∞·ª£ng ·ªü m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c
- C√≥ th·ªÉ c·∫ßn fine-tune threshold

### EER cao (> 10%)
- ‚ùå Ch·∫•t l∆∞·ª£ng k√©m
- C·∫ßn c·∫£i thi·ªán model ho·∫∑c features

### F1 Score
- **Best F1 > 95%**: Xu·∫•t s·∫Øc
- **Best F1 = 90-95%**: T·ªët
- **Best F1 < 90%**: C·∫ßn c·∫£i thi·ªán

### AUC Score
- **AUC > 0.95**: Xu·∫•t s·∫Øc
- **AUC = 0.90-0.95**: T·ªët
- **AUC = 0.80-0.90**: Trung b√¨nh
- **AUC < 0.80**: K√©m

## ‚ö° Optimization Features

### 1. Single Embedding Extraction
Extract embeddings **ch·ªâ 1 l·∫ßn** cho m·ªói file:
```python
# Extract t·∫•t c·∫£ embeddings t·ª´ c√°c model instances c√πng l√∫c
result, _ = pipeline._extract_embeddings(file_path, max_speakers=1)
emb_cache[file_path] = {
    "pyannote": result["pyan_community_embeddings"][0],
    "speechbrain": result["sb_default_embeddings"][0],
    "nemo_titanet": result["nemo_tianet_embeddings"][0],
    "nemo_ecapa_tdnn": result["nemo_ecapa_tdnn_embeddings"][0]
}
```

### 2. Efficient Trial Processing
```python
# Ch·ªâ extract unique files
all_files = set()
for p1, p2, _ in trials:
    all_files.add(p1)
    all_files.add(p2)
```

### 3. Progress Tracking
```python
# Progress bar cho embedding extraction
for fpath in tqdm(list(all_files), desc="Extracting embeddings"):
    ...
```

---

# üí° Tips & Best Practices

## ASR Evaluation
1. **Checkpoint regularly**: Script t·ª± ƒë·ªông save checkpoint, ƒë·ª´ng x√≥a file
2. **VAD filtering**: Test c·∫£ 2 modes (with/without VAD)
3. **RTF analysis**: Monitor RTF ƒë·ªÉ ƒë·∫£m b·∫£o realtime performance
4. **Batch processing**: S·ª≠ d·ª•ng batch cho API-based models (Gemini, VLLM)
5. **Error handling**: Script c√≥ retry mechanism cho API calls

## Diarization Evaluation
1. **Dataset quality**: ƒê·∫£m b·∫£o audio quality t·ªët v√† speakers ƒë·ªß diverse
2. **Trial balance**: C√¢n b·∫±ng s·ªë genuine v√† impostor pairs
3. **Threshold selection**: 
   - D√πng threshold t·∫°i EER cho balanced performance
   - D√πng threshold t·∫°i Best F1 cho maximum accuracy
4. **Visualization**: Xem curves ƒë·ªÉ understand model behavior
5. **Embedding comparison**: So s√°nh gi·ªØa c√°c models (Pyannote, SpeechBrain, NeMo variants) ƒë·ªÉ ch·ªçn best model
6. **Multiple instances**: S·ª≠ d·ª•ng multiple instances c·ªßa c√πng model type v·ªõi configs kh√°c nhau ƒë·ªÉ ƒë√°nh gi√° v√† so s√°nh
7. **Cache management**: Script c√≥ caching mechanism ƒë·ªÉ tr√°nh re-extract embeddings

---

# üîó Tham kh·∫£o

## ASR
- [jiwer documentation](https://github.com/jitsi/jiwer)
- [Faster Whisper](https://github.com/guillaumekln/faster-whisper)
- [FunASR](https://github.com/alibaba-damo-academy/FunASR)
- [WER explanation](https://en.wikipedia.org/wiki/Word_error_rate)

## Speaker Verification
- [scikit-learn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [ROC Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
- [DET Curve](https://en.wikipedia.org/wiki/Detection_error_tradeoff)
- [EER explanation](https://www.sciencedirect.com/topics/computer-science/equal-error-rate)

## Models
- [Pyannote Audio](https://github.com/pyannote/pyannote-audio)
- [SpeechBrain](https://github.com/speechbrain/speechbrain)
- [NVIDIA NeMo](https://github.com/NVIDIA/NeMo)
  - [TitaNet](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_recognition/models.html#titanet)
  - [ECAPA-TDNN](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_recognition/models.html#ecapa-tdnn)
- [Google Gemini](https://ai.google.dev/)
- [vLLM](https://github.com/vllm-project/vllm)
