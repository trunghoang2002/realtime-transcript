# ğŸ“Š Evaluation Scripts Documentation

Há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cho 2 tÃ¡c vá»¥ chÃ­nh:
1. **ASR Evaluation** (`eval_asr.py`): ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng nháº­n dáº¡ng giá»ng nÃ³i (Speech Recognition)
2. **Diarization Evaluation** (`eval_diarization.py`): ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng phÃ¢n biá»‡t ngÆ°á»i nÃ³i (Speaker Verification)

---

# ğŸ¤ Part 1: ASR Evaluation

## ğŸ“‹ Má»¥c Ä‘Ã­ch

Script `eval_asr.py` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ:
- ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c model ASR (Whisper, SenseVoice, Gemini, VLLM)
- Äo lÆ°á»ng hiá»‡u suáº¥t xá»­ lÃ½ thá»i gian thá»±c (Real-Time Factor - RTF)
- So sÃ¡nh hiá»‡u suáº¥t giá»¯a cÃ¡c model khÃ¡c nhau
- ÄÃ¡nh giÃ¡ vá»›i/khÃ´ng cÃ³ VAD filtering
- Há»— trá»£ checkpoint Ä‘á»ƒ tiáº¿p tá»¥c evaluation khi bá»‹ giÃ¡n Ä‘oáº¡n

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Base Class: `BaseASR`
Abstract class Ä‘á»‹nh nghÄ©a interface chung cho táº¥t cáº£ ASR models:
- `_transcribe_no_vad(audio)`: Transcribe khÃ´ng dÃ¹ng VAD
- `_transcribe_with_vad(audio)`: Transcribe cÃ³ dÃ¹ng VAD
- `transcribe(file_path, vad_filter)`: Public API

### Supported ASR Models

#### 1. **WhisperJA** (Faster Whisper)
```python
transcriber = WhisperJA(
    model_name="large-v3",  # tiny, base, small, medium, large, large-v1, large-v2, large-v3, turbo
    device="cuda",
    compute_type="float16",
    beam_size=5,
    best_of=5,
    temperature=0.0
)
```

**Features:**
- Há»— trá»£ multiple model sizes
- Tá»‘i Æ°u vá»›i faster-whisper (CTranslate2)
- VAD filtering vá»›i Silero VAD
- Beam search vÃ  temperature control

#### 2. **SenseVoiceJA** (FunASR)
```python
transcriber = SenseVoiceJA(
    model_name="iic/SenseVoiceSmall",
    device="cuda",
    max_single_segment_time=30000,
    batch_size_s=20,
    use_itn=False
)
```

**Features:**
- Optimized cho tiáº¿ng Nháº­t
- Batch processing
- Inverse Text Normalization (ITN)

#### 3. **GeminiASR** (Google Gemini)
```python
transcriber = GeminiASR(
    api_key=os.getenv("GEMINI_API_KEY"),
    model_name="gemini-2.5-pro"  # gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro
)
```

**Features:**
- Cloud-based ASR
- Há»— trá»£ multiple languages
- Automatic retry vá»›i exponential backoff
- Temperature=0.0 cho reproducibility

#### 4. **VllmASR** (vLLM Server)
```python
transcriber = VllmASR(
    base_url="https://your-vllm-server.com/v1",
    api_key="EMPTY",
    model_name="Qwen/Qwen2-Audio-7B-Instruct",
    prompt="Transcribe the audio into text."
)
```

**Features:**
- OpenAI-compatible API
- Custom prompt engineering
- Base64 audio encoding
- Exponential backoff retry

## ğŸ“Š Metrics

### 1. WER (Word Error Rate)
- **Äá»‹nh nghÄ©a**: Tá»· lá»‡ lá»—i tá»«
- **CÃ´ng thá»©c**: `WER = (S + D + I) / N`
  - S: Sá»‘ tá»« bá»‹ thay tháº¿ (Substitutions)
  - D: Sá»‘ tá»« bá»‹ xÃ³a (Deletions)
  - I: Sá»‘ tá»« bá»‹ thÃªm vÃ o (Insertions)
  - N: Tá»•ng sá»‘ tá»« trong ground truth
- **GiÃ¡ trá»‹**: 0.0 = hoÃ n háº£o, cÃ ng cao cÃ ng kÃ©m
- **Tokenization**: Sá»­ dá»¥ng Sudachi tokenizer cho tiáº¿ng Nháº­t

### 2. CER (Character Error Rate)
- **Äá»‹nh nghÄ©a**: Tá»· lá»‡ lá»—i kÃ½ tá»±
- **CÃ´ng thá»©c**: `CER = (S + D + I) / N` (tÃ­nh theo kÃ½ tá»±)
- **GiÃ¡ trá»‹**: 0.0 = hoÃ n háº£o, cÃ ng cao cÃ ng kÃ©m
- **Há»¯u Ã­ch cho**: NgÃ´n ngá»¯ khÃ´ng cÃ³ khoáº£ng tráº¯ng (tiáº¿ng Nháº­t, tiáº¿ng Trung)

### 3. RTF (Real-Time Factor)
- **Äá»‹nh nghÄ©a**: Tá»· lá»‡ giá»¯a thá»i gian xá»­ lÃ½ vÃ  Ä‘á»™ dÃ i audio
- **CÃ´ng thá»©c**: `RTF = processing_time / audio_duration`
- **GiÃ¡ trá»‹**: 
  - RTF < 1.0: Xá»­ lÃ½ nhanh hÆ¡n realtime (tá»‘t)
  - RTF = 1.0: Xá»­ lÃ½ Ä‘Ãºng báº±ng realtime
  - RTF > 1.0: Xá»­ lÃ½ cháº­m hÆ¡n realtime (khÃ´ng Ä‘á»§ cho realtime)

## ğŸ”„ Text Normalization

TrÆ°á»›c khi tÃ­nh WER/CER, text Ä‘Æ°á»£c normalize:

```python
def eval_score(ground_truth, prediction):
    # 1. Lowercase
    ground_truth = ground_truth.lower()
    prediction = prediction.lower()
    
    # 2. Remove punctuation (Japanese)
    pattern = r"[\p{P}ï½~ï¼‹ï¼ï¼„|]+"
    ground_truth = re.sub(pattern, "", ground_truth)
    prediction = re.sub(pattern, "", prediction)
    
    # 3. Remove tags
    prediction = prediction.replace("<[^>]*>", "")
    
    # 4. Remove English/spaces
    ground_truth = re.sub(r"[A-Za-z\s]+", "", ground_truth).strip()
    prediction = re.sub(r"[A-Za-z\s]+", "", prediction).strip()
    
    # 5. Calculate metrics with Japanese tokenizer
    wer_score = wer(ground_truth, prediction, 
                    reference_transform=wer_ja, 
                    hypothesis_transform=wer_ja)
    cer_score = cer(ground_truth, prediction)
    return wer_score, cer_score
```

## ğŸ’¾ Checkpoint Mechanism

Script há»— trá»£ checkpoint Ä‘á»ƒ tiáº¿p tá»¥c evaluation khi bá»‹ giÃ¡n Ä‘oáº¡n:

```python
results_file = "eval_results_checkpoint.csv"
```

**Features:**
- Tá»± Ä‘á»™ng lÆ°u káº¿t quáº£ sau má»—i test case
- Resume tá»« file checkpoint náº¿u script bá»‹ dá»«ng
- Track completed files Ä‘á»ƒ khÃ´ng xá»­ lÃ½ láº¡i

**CSV Format:**
```csv
file_path,ground_truth,prediction,wer_score,cer_score,rtf,audio_duration,processing_time
```

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Chuáº©n bá»‹ dataset
Dataset CSV vá»›i format:
```csv
...,file_path,ground_truth
...,audio/file1.wav,ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™
...,audio/file2.wav,éŸ³å£°èªè­˜ã®è©•ä¾¡
```

### 2. Cáº¥u hÃ¬nh transcriber
Uncomment model báº¡n muá»‘n test trong `eval_asr.py`:

```python
# Option 1: Whisper
transcriber = WhisperJA(
    model_name="large-v1",
    device="cuda",
    compute_type="float16"
)

# Option 2: SenseVoice
# transcriber = SenseVoiceJA(...)

# Option 3: Gemini
# transcriber = GeminiASR(...)

# Option 4: VLLM
# transcriber = VllmASR(...)
```

### 3. Cháº¡y evaluation
```bash
cd backend/eval
python eval_asr.py
```

### 4. Xem káº¿t quáº£
```
Total test cases: 400
Remaining: 350
Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400
==================================================
FINAL RESULTS
==================================================
Total evaluated: 400 test cases
WER: 0.1234
CER: 0.0567
RTF: 0.4567 (mean)
RTF: 0.4123 (median)
RTF: 0.2100 (min) | 0.9800 (max)
```

---

# ğŸ‘¥ Part 2: Diarization Evaluation

## ğŸ“‹ Má»¥c Ä‘Ã­ch

Script `eval_diarization.py` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ:
- ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng speaker verification/diarization
- So sÃ¡nh 2 loáº¡i embeddings: Pyannote vÃ  SpeechBrain
- TÃ­nh toÃ¡n multiple metrics: EER, FAR, FRR, Precision, Recall, F1, AUC
- Visualize ROC curves, DET curves, Precision-Recall curves
- TÃ¬m optimal threshold cho tá»«ng metric

## ğŸ”§ Dependencies

```bash
pip install scikit-learn
pip install matplotlib
pip install tqdm
pip install numpy
```

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Pipeline
```python
from fusion_diarization import RealtimeSpeakerDiarization
pipeline = RealtimeSpeakerDiarization()
```

Pipeline extract 2 loáº¡i embeddings:
- **Pyannote embeddings**: From pyannote.audio
- **SpeechBrain embeddings**: From SpeechBrain ECAPA-TDNN

### Evaluation Process

1. **List speakers and utterances**: Scan dataset folder
2. **Build trials**: Táº¡o genuine pairs (cÃ¹ng speaker) vÃ  impostor pairs (khÃ¡c speaker)
3. **Extract embeddings**: Extract 1 láº§n cho táº¥t cáº£ files
4. **Compute scores**: TÃ­nh cosine similarity cho tá»«ng trial
5. **Calculate metrics**: EER, FAR, FRR, Precision, Recall, F1, AUC
6. **Visualize**: Váº½ vÃ  lÆ°u ROC, DET, PR curves

## ğŸ“Š Metrics

### 1. EER (Equal Error Rate)
- **Äá»‹nh nghÄ©a**: Äiá»ƒm mÃ  FAR = FRR
- **GiÃ¡ trá»‹**: CÃ ng tháº¥p cÃ ng tá»‘t (0% = hoÃ n háº£o)
- **Ã nghÄ©a**: CÃ¢n báº±ng giá»¯a false accept vÃ  false reject

### 2. FAR (False Acceptance Rate)
- **Äá»‹nh nghÄ©a**: Tá»· lá»‡ impostor pairs bá»‹ accept nháº§m
- **CÃ´ng thá»©c**: `FAR = FP / (FP + TN)`
- **GiÃ¡ trá»‹**: CÃ ng tháº¥p cÃ ng tá»‘t

### 3. FRR (False Rejection Rate)
- **Äá»‹nh nghÄ©a**: Tá»· lá»‡ genuine pairs bá»‹ reject nháº§m
- **CÃ´ng thá»©c**: `FRR = FN / (FN + TP)`
- **GiÃ¡ trá»‹**: CÃ ng tháº¥p cÃ ng tá»‘t

### 4. Precision
- **CÃ´ng thá»©c**: `Precision = TP / (TP + FP)`
- **GiÃ¡ trá»‹**: 0.0 - 1.0 (cÃ ng cao cÃ ng tá»‘t)

### 5. Recall (Sensitivity, TPR)
- **CÃ´ng thá»©c**: `Recall = TP / (TP + FN)`
- **GiÃ¡ trá»‹**: 0.0 - 1.0 (cÃ ng cao cÃ ng tá»‘t)

### 6. F1 Score
- **CÃ´ng thá»©c**: `F1 = 2 * (Precision * Recall) / (Precision + Recall)`
- **GiÃ¡ trá»‹**: 0.0 - 1.0 (cÃ ng cao cÃ ng tá»‘t)

### 7. AUC (Area Under ROC Curve)
- **Äá»‹nh nghÄ©a**: Diá»‡n tÃ­ch dÆ°á»›i ROC curve
- **GiÃ¡ trá»‹**: 0.0 - 1.0 (1.0 = perfect classifier)

## ğŸ“ Dataset Structure

Dataset cáº§n tuÃ¢n theo cáº¥u trÃºc:
```
dataset/
â”œâ”€â”€ speaker_001/
â”‚   â”œâ”€â”€ falset10/
â”‚   â”‚   â””â”€â”€ wav24kHz16bit/
â”‚   â”‚       â”œâ”€â”€ file1.wav
â”‚   â”‚       â””â”€â”€ file2.wav
â”‚   â”œâ”€â”€ nonpara30/
â”‚   â”‚   â””â”€â”€ wav24kHz16bit/
â”‚   â”œâ”€â”€ parallel100/
â”‚   â”‚   â””â”€â”€ wav24kHz16bit/
â”‚   â””â”€â”€ whisper10/
â”‚       â””â”€â”€ wav24kHz16bit/
â”œâ”€â”€ speaker_002/
â”‚   â””â”€â”€ ...
```

**YÃªu cáº§u:**
- Má»—i speaker cÃ³ Ã­t nháº¥t 2 utterances
- 4 loáº¡i folder: `falset10`, `nonpara30`, `parallel100`, `whisper10`
- Audio files trong `wav24kHz16bit/`

## ğŸ¯ Trial Generation

### Genuine Pairs (label=1)
- Cáº·p 2 utterances khÃ¡c nhau cá»§a cÃ¹ng 1 speaker
- Max pairs per speaker: `max_genuine_per_spk=50`

### Impostor Pairs (label=0)
- Cáº·p utterances tá»« 2 speakers khÃ¡c nhau
- Pairs per speaker: `impostor_per_spk=100`

### Example
```python
trials = build_trials(
    spk2utts,
    max_genuine_per_spk=50,  # Max 50 genuine pairs má»—i speaker
    impostor_per_spk=100     # 100 impostor pairs má»—i speaker
)
# Output: [(path1, path2, label), ...]
```

## ğŸ¨ Visualization

Script tá»± Ä‘á»™ng táº¡o 3 loáº¡i biá»ƒu Ä‘á»“ trong folder `eval_results/`:

### 1. ROC Curve (`roc_curves.png`)
- **Trá»¥c X**: False Positive Rate (FAR)
- **Trá»¥c Y**: True Positive Rate (1 - FRR)
- **Features**:
  - So sÃ¡nh Pyannote vs SpeechBrain
  - Hiá»ƒn thá»‹ AUC score
  - ÄÃ¡nh dáº¥u Ä‘iá»ƒm EER
  - ÄÆ°á»ng baseline (random classifier)

### 2. DET Curve (`det_curves.png`)
- **Trá»¥c X**: False Acceptance Rate (%)
- **Trá»¥c Y**: False Rejection Rate (%)
- **Features**:
  - Dá»… nhÃ¬n hÆ¡n cho speaker verification
  - ÄÃ¡nh dáº¥u Ä‘iá»ƒm EER
  - ÄÆ°á»ng chÃ©o FAR=FRR

### 3. Precision-Recall Curve (`precision_recall_curves.png`)
- **Trá»¥c X**: Recall
- **Trá»¥c Y**: Precision
- **Features**:
  - Hiá»ƒn thá»‹ PR AUC
  - ÄÃ¡nh dáº¥u Ä‘iá»ƒm Best F1
  - So sÃ¡nh 2 embeddings

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Chuáº©n bá»‹ dataset
Organize audio files theo cáº¥u trÃºc folder nhÆ° trÃªn.

### 2. Cáº¥u hÃ¬nh (optional)
Trong `eval_diarization.py`, cÃ³ thá»ƒ Ä‘iá»u chá»‰nh:
```python
# Sá»‘ lÆ°á»£ng trials
max_genuine_per_spk = 50
impostor_per_spk = 100

# Output directory cho biá»ƒu Ä‘á»“
output_dir = "eval_results"

# Dataset path
dataset_path = "dataset/jvs_ver1"
```

### 3. Cháº¡y evaluation
```bash
cd backend/eval
python eval_diarization.py
```

### 4. Xem káº¿t quáº£

**Console Output:**
```
Found 100 speakers usable.
Total trials: 15000
Extracting embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500

=== Evaluating pyannote embeddings ===
Computing metrics on 14850 valid trials
EER: 5.23% | FAR@EER: 5.21% | FRR@EER: 5.25% | Thr(EER): 0.6234
Precision@EER: 94.77% | Recall@EER: 94.75% | F1@EER: 94.76%
Best F1: 96.12% | Precision@F1: 95.89% | Recall@F1: 96.35% | Thr(F1): 0.6789
AUC: 0.9876

=== Evaluating speechbrain embeddings ===
Computing metrics on 14850 valid trials
EER: 4.87% | FAR@EER: 4.85% | FRR@EER: 4.89% | Thr(EER): 0.7123
Precision@EER: 95.13% | Recall@EER: 95.11% | F1@EER: 95.12%
Best F1: 96.87% | Precision@F1: 96.45% | Recall@F1: 97.29% | Thr(F1): 0.7456
AUC: 0.9912

=== Plotting curves ===
ROC curve saved to: eval_results/roc_curves.png
DET curve saved to: eval_results/det_curves.png
Precision-Recall curve saved to: eval_results/precision_recall_curves.png
```

**Generated Files:**
- `eval_results/roc_curves.png`
- `eval_results/det_curves.png`
- `eval_results/precision_recall_curves.png`

## ğŸ›¡ï¸ Error Handling

Script xá»­ lÃ½ robust vá»›i cÃ¡c edge cases:

### 1. Embedding Extraction Failures
```python
# Skip náº¿u:
- result is None
- embeddings is None or empty
- embeddings toÃ n báº±ng 0
- embeddings toÃ n lÃ  NaN
```

### 2. Score Computation Failures
```python
# Skip trial náº¿u:
- File khÃ´ng cÃ³ trong cache
- Embedding type khÃ´ng tá»“n táº¡i
- Embedding lÃ  None
- Embedding toÃ n 0 hoáº·c NaN
- Cosine score lÃ  NaN hoáº·c inf
```

### 3. Metrics Calculation
```python
# Return None náº¿u:
- KhÃ´ng cÃ³ valid trials
- zero_division=0 trong precision_recall_fscore_support
```

## ğŸ” Giáº£i thÃ­ch káº¿t quáº£

### EER tháº¥p (< 5%)
- âœ… Cháº¥t lÆ°á»£ng speaker verification xuáº¥t sáº¯c
- System phÃ¢n biá»‡t speakers ráº¥t chÃ­nh xÃ¡c

### EER trung bÃ¬nh (5% - 10%)
- âš ï¸ Cháº¥t lÆ°á»£ng á»Ÿ má»©c cháº¥p nháº­n Ä‘Æ°á»£c
- CÃ³ thá»ƒ cáº§n fine-tune threshold

### EER cao (> 10%)
- âŒ Cháº¥t lÆ°á»£ng kÃ©m
- Cáº§n cáº£i thiá»‡n model hoáº·c features

### F1 Score
- **Best F1 > 95%**: Xuáº¥t sáº¯c
- **Best F1 = 90-95%**: Tá»‘t
- **Best F1 < 90%**: Cáº§n cáº£i thiá»‡n

### AUC Score
- **AUC > 0.95**: Xuáº¥t sáº¯c
- **AUC = 0.90-0.95**: Tá»‘t
- **AUC = 0.80-0.90**: Trung bÃ¬nh
- **AUC < 0.80**: KÃ©m

## âš¡ Optimization Features

### 1. Single Embedding Extraction
Extract embeddings **chá»‰ 1 láº§n** cho má»—i file:
```python
# Extract cáº£ 2 loáº¡i embeddings cÃ¹ng lÃºc
result, _, _ = pipeline._extract_embeddings(file_path, max_speakers=1)
emb_cache[file_path] = {
    "pyannote": result["pyannote_embeddings"][0],
    "speechbrain": result["speechbrain_embeddings"][0]
}
```

### 2. Efficient Trial Processing
```python
# Chá»‰ extract unique files
all_files = set()
for p1, p2, _ in trials:
    all_files.add(p1)
    all_files.add(p2)
```

### 3. Progress Tracking
```python
# Progress bar cho embedding extraction
for fpath in tqdm(list(all_files), desc="Extracting embeddings"):
    ...
```

---

# ğŸ’¡ Tips & Best Practices

## ASR Evaluation
1. **Checkpoint regularly**: Script tá»± Ä‘á»™ng save checkpoint, Ä‘á»«ng xÃ³a file
2. **VAD filtering**: Test cáº£ 2 modes (with/without VAD)
3. **RTF analysis**: Monitor RTF Ä‘á»ƒ Ä‘áº£m báº£o realtime performance
4. **Batch processing**: Sá»­ dá»¥ng batch cho API-based models (Gemini, VLLM)
5. **Error handling**: Script cÃ³ retry mechanism cho API calls

## Diarization Evaluation
1. **Dataset quality**: Äáº£m báº£o audio quality tá»‘t vÃ  speakers Ä‘á»§ diverse
2. **Trial balance**: CÃ¢n báº±ng sá»‘ genuine vÃ  impostor pairs
3. **Threshold selection**: 
   - DÃ¹ng threshold táº¡i EER cho balanced performance
   - DÃ¹ng threshold táº¡i Best F1 cho maximum accuracy
4. **Visualization**: Xem curves Ä‘á»ƒ understand model behavior
5. **Embedding comparison**: So sÃ¡nh Pyannote vs SpeechBrain Ä‘á»ƒ chá»n best model

---

# ğŸ”— Tham kháº£o

## ASR
- [jiwer documentation](https://github.com/jitsi/jiwer)
- [Faster Whisper](https://github.com/guillaumekln/faster-whisper)
- [FunASR](https://github.com/alibaba-damo-academy/FunASR)
- [WER explanation](https://en.wikipedia.org/wiki/Word_error_rate)

## Speaker Verification
- [scikit-learn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [ROC Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
- [DET Curve](https://en.wikipedia.org/wiki/Detection_error_tradeoff)
- [EER explanation](https://www.sciencedirect.com/topics/computer-science/equal-error-rate)

## Models
- [Pyannote Audio](https://github.com/pyannote/pyannote-audio)
- [SpeechBrain](https://github.com/speechbrain/speechbrain)
- [Google Gemini](https://ai.google.dev/)
- [vLLM](https://github.com/vllm-project/vllm)
