# Patch SpeechBrain compatibility issue vá»›i huggingface_hub
import fix_speechbrain  # Pháº£i import TRÆ¯á»šC speechbrain
from speechbrain.inference import EncoderClassifier
import torch
import numpy as np
from typing import Optional, Dict, List, Callable, Union
from scipy.spatial.distance import cdist
from get_audio import decode_audio

class RealtimeSpeakerDiarization():
    """Realtime Speaker Diarization Pipeline with persistent speaker embeddings"""
    
    def __init__(self, similarity_threshold=0.7,  # threshold Ä‘á»ƒ match speaker
                 embedding_update_weight=0.3,  # trá»ng sá»‘ cáº­p nháº­t embedding má»›i
                 min_similarity_gap=0.3,  # gap tá»‘i thiá»ƒu Ä‘á»ƒ match (náº¿u ná»•i báº­t)
                 preloaded_model=None,  # Optional: dÃ¹ng model Ä‘Ã£ preload Ä‘á»ƒ trÃ¡nh load láº¡i
                 *args, **kwargs):

        # Context persistence cho realtime
        self.speaker_memory: Dict[str, np.ndarray] = {}  # {speaker_id: EMA embedding}
        self.speaker_embedding_clusters: Dict[str, List[np.ndarray]] = {}  # {speaker_id: [embeddings]}
        self.speaker_counts: Dict[str, int] = {}  # sá»‘ láº§n xuáº¥t hiá»‡n cá»§a má»—i speaker
        self.speaker_history: List[Dict] = []  # lá»‹ch sá»­ diarization
        self.similarity_threshold = similarity_threshold
        self.embedding_update_weight = embedding_update_weight
        self.min_similarity_gap = min_similarity_gap  # Gap threshold cho distinctive matching
        self.max_cluster_size = 20  # Giá»›i háº¡n sá»‘ embeddings trong cluster
        self.total_chunks_processed = 0
        self.next_speaker_id = 0
        self.embedding_metric = "cosine"

        # Load speaker embedding model (hoáº·c dÃ¹ng preloaded)
        if preloaded_model is not None:
            self.spk_model = preloaded_model
        else:
            self.spk_model = EncoderClassifier.from_hparams(
                source="speechbrain/spkrec-ecapa-voxceleb",
                run_opts={"device": "cuda" if torch.cuda.is_available() else "cpu"}
            )
    
    def reset_context(self):
        """Reset toÃ n bá»™ context - dÃ¹ng khi báº¯t Ä‘áº§u conversation má»›i"""
        self.speaker_memory.clear()
        self.speaker_embedding_clusters.clear()
        self.speaker_counts.clear()
        self.speaker_history.clear()
        self.total_chunks_processed = 0
        self.next_speaker_id = 0
        
    def _match_speakers_with_memory(self, new_embeddings: np.ndarray, 
                                    new_labels: List[str],
                                    max_speakers: Optional[int] = None) -> Dict[str, str]:
        """
        Match speakers má»›i vá»›i speakers Ä‘Ã£ biáº¿t trong memory
        Sá»­ dá»¥ng 2-tier matching: EMA embedding (fast) vÃ  cluster centroid (robust)
        
        Parameters
        ----------
        new_embeddings : (num_speakers, dimension) array
            Embeddings cá»§a speakers trong chunk hiá»‡n táº¡i
        new_labels : List[str]
            Labels táº¡m thá»i cá»§a speakers má»›i
        max_speakers : int, optional
            Maximum number of speakers allowed. If reached, force-assign to best match.
            
        Returns
        -------
        mapping : Dict[str, str]
            Mapping tá»« label táº¡m thá»i -> speaker_id Ä‘Ã£ biáº¿t hoáº·c má»›i
        """
        if len(self.speaker_memory) == 0:
            # ChÆ°a cÃ³ speaker nÃ o trong memory
            mapping = {}
            for i, label in enumerate(new_labels):
                print(f"Creating new speaker: {label} with id: {self.next_speaker_id:02d}")
                speaker_id = f"SPEAKER_{self.next_speaker_id:02d}"
                self.next_speaker_id += 1
                mapping[label] = speaker_id
                
                # Initialize EMA embedding vÃ  cluster
                self.speaker_memory[speaker_id] = new_embeddings[i].copy()
                self.speaker_embedding_clusters[speaker_id] = [new_embeddings[i].copy()]
                self.speaker_counts[speaker_id] = 1
            return mapping
        
        # CÃ³ speakers trong memory - tÃ­nh similarity
        memory_speaker_ids = list(self.speaker_memory.keys())
        memory_embeddings = np.array([self.speaker_memory[sid] for sid in memory_speaker_ids])
        
        # TÃ­nh cosine similarity hoáº·c euclidean distance
        if self.embedding_metric == "cosine":
            # Cosine similarity (1 - cosine distance)
            distances = cdist(new_embeddings, memory_embeddings, metric='cosine')
            similarities_ema = 1 - distances
        else:
            # Euclidean distance -> similarity
            distances = cdist(new_embeddings, memory_embeddings, metric='euclidean')
            similarities_ema = 1 / (1 + distances)
        
        mapping = {}
        used_memory_speakers = set()
        
        # Greedy matching vá»›i 2-tier approach
        for i, label in enumerate(new_labels):
            new_embedding = new_embeddings[i]
            
            # Tier 1: So sÃ¡nh vá»›i EMA embeddings (fast path)
            best_match_idx = np.argmax(similarities_ema[i])
            best_similarity_ema = similarities_ema[i, best_match_idx]
            best_speaker_id = memory_speaker_ids[best_match_idx]
            
            # Find second best for gap analysis
            sorted_indices = np.argsort(similarities_ema[i])[::-1]  # Descending order
            second_best_similarity_ema = similarities_ema[i, sorted_indices[1]] if len(sorted_indices) > 1 else -1
            similarity_gap_ema = best_similarity_ema - second_best_similarity_ema
            
            print(f"\n[TIER 1] Label: {label}")
            print(f"  Best EMA similarity: {best_similarity_ema:.3f} with {best_speaker_id}")
            print(f"  Second best similarity: {second_best_similarity_ema:.3f}")
            print(f"  Gap: {similarity_gap_ema:.3f}")
            print(f"  Threshold: {self.similarity_threshold}")
            
            matched_speaker_id = None
            
            # Check matching conditions
            if best_speaker_id not in used_memory_speakers:
                if best_similarity_ema >= self.similarity_threshold:
                    # Match via threshold
                    matched_speaker_id = best_speaker_id
                    print(f"  âœ… Matched via EMA (threshold)!")
                elif similarity_gap_ema > self.min_similarity_gap and second_best_similarity_ema > 0:
                    # Match via significant gap
                    matched_speaker_id = best_speaker_id
                    print(f"  âœ… Matched via EMA (significant gap > {self.min_similarity_gap})!")
                
            if matched_speaker_id is None:
                # Tier 2: So sÃ¡nh vá»›i cluster centroids (robust path)
                print(f"  âŒ EMA not matched, trying cluster centroids...")
                
                cluster_similarities = []
                cluster_speaker_ids = []
                
                for speaker_id in memory_speaker_ids:
                    if speaker_id in used_memory_speakers:
                        continue
                    
                    # TÃ­nh centroid cá»§a cluster
                    cluster = self.speaker_embedding_clusters[speaker_id]
                    if len(cluster) > 0:
                        cluster_array = np.array(cluster)
                        centroid = np.mean(cluster_array, axis=0)
                        
                        # Normalize centroid náº¿u dÃ¹ng cosine
                        if self.embedding_metric == "cosine":
                            centroid = centroid / np.linalg.norm(centroid)
                        
                        # TÃ­nh similarity vá»›i centroid
                        if self.embedding_metric == "cosine":
                            cluster_similarity = 1 - cdist([new_embedding], [centroid], metric='cosine')[0, 0]
                        else:
                            cluster_similarity = 1 / (1 + cdist([new_embedding], [centroid], metric='euclidean')[0, 0])
                        
                        print(f"  [TIER 2] Cluster centroid similarity with {speaker_id}: {cluster_similarity:.3f}")
                        
                        cluster_similarities.append(cluster_similarity)
                        cluster_speaker_ids.append(speaker_id)
                
                # Find best and second best for gap analysis
                if len(cluster_similarities) > 0:
                    sorted_cluster_indices = np.argsort(cluster_similarities)[::-1]
                    best_cluster_idx = sorted_cluster_indices[0]
                    best_cluster_similarity = cluster_similarities[best_cluster_idx]
                    best_cluster_speaker_id = cluster_speaker_ids[best_cluster_idx]
                    
                    second_best_cluster_similarity = (
                        cluster_similarities[sorted_cluster_indices[1]] 
                        if len(sorted_cluster_indices) > 1 
                        else -1
                    )
                    cluster_similarity_gap = best_cluster_similarity - second_best_cluster_similarity
                    
                    print(f"  [TIER 2] Best: {best_cluster_similarity:.3f}, Second: {second_best_cluster_similarity:.3f}, Gap: {cluster_similarity_gap:.3f}")
                    
                    # Check matching conditions for cluster
                    if best_cluster_similarity >= self.similarity_threshold:
                        matched_speaker_id = best_cluster_speaker_id
                        print(f"  âœ… Matched via cluster centroid (threshold) with {matched_speaker_id}!")
                    elif cluster_similarity_gap > self.min_similarity_gap and second_best_cluster_similarity > 0:
                        matched_speaker_id = best_cluster_speaker_id
                        print(f"  âœ… Matched via cluster centroid (significant gap > {self.min_similarity_gap}) with {matched_speaker_id}!")
            
            # Xá»­ lÃ½ káº¿t quáº£ matching
            if matched_speaker_id is not None:
                # Match thÃ nh cÃ´ng
                mapping[label] = matched_speaker_id
                used_memory_speakers.add(matched_speaker_id)
                
                # Cáº­p nháº­t EMA embedding
                old_embedding = self.speaker_memory[matched_speaker_id]
                updated_embedding = (1 - self.embedding_update_weight) * old_embedding + \
                                   self.embedding_update_weight * new_embedding
                # Normalize náº¿u dÃ¹ng cosine similarity
                if self.embedding_metric == "cosine":
                    updated_embedding = updated_embedding / np.linalg.norm(updated_embedding)
                self.speaker_memory[matched_speaker_id] = updated_embedding
                
                # Add vÃ o cluster (vá»›i limit size)
                cluster = self.speaker_embedding_clusters[matched_speaker_id]
                cluster.append(new_embedding.copy())
                # Keep only recent embeddings
                if len(cluster) > self.max_cluster_size:
                    self.speaker_embedding_clusters[matched_speaker_id] = cluster[-self.max_cluster_size:]
                
                self.speaker_counts[matched_speaker_id] += 1
                print(f"  ğŸ“Š Updated: EMA + added to cluster (size: {len(self.speaker_embedding_clusters[matched_speaker_id])})")
                
            else:
                # KhÃ´ng match Ä‘Æ°á»£c - check max_speakers constraint
                current_num_speakers = len(self.speaker_memory)
                
                if max_speakers is not None and current_num_speakers >= max_speakers:
                    # ÄÃƒ Äáº T max_speakers - Force assign vÃ o speaker cÃ³ similarity cao nháº¥t
                    print(f"  âš ï¸  Max speakers ({max_speakers}) reached! Force-assigning to best match...")
                    
                    # TÃ¬m speaker cÃ³ similarity cao nháº¥t (dÃ¹ < threshold)
                    best_overall_similarity = -1
                    best_overall_speaker_id = None
                    
                    # Check both EMA and cluster similarities
                    for speaker_idx, speaker_id in enumerate(memory_speaker_ids):
                        if speaker_id in used_memory_speakers:
                            continue
                        
                        # Get EMA similarity
                        ema_sim = similarities_ema[i, speaker_idx]
                        
                        # Get cluster centroid similarity
                        cluster = self.speaker_embedding_clusters[speaker_id]
                        if len(cluster) > 0:
                            cluster_array = np.array(cluster)
                            centroid = np.mean(cluster_array, axis=0)
                            if self.embedding_metric == "cosine":
                                centroid = centroid / np.linalg.norm(centroid)
                            
                            if self.embedding_metric == "cosine":
                                cluster_sim = 1 - cdist([new_embedding], [centroid], metric='cosine')[0, 0]
                            else:
                                cluster_sim = 1 / (1 + cdist([new_embedding], [centroid], metric='euclidean')[0, 0])
                        else:
                            cluster_sim = ema_sim
                        
                        # # Use max of EMA and cluster similarity
                        max_sim = max(ema_sim*1.3, cluster_sim)
                        
                        if max_sim > best_overall_similarity:
                            best_overall_similarity = max_sim
                            best_overall_speaker_id = speaker_id
                    
                    if best_overall_speaker_id is not None:
                        matched_speaker_id = best_overall_speaker_id
                        mapping[label] = matched_speaker_id
                        used_memory_speakers.add(matched_speaker_id)
                        
                        print(f"  ğŸ”€ Force-assigned to {matched_speaker_id} (similarity: {best_overall_similarity:.3f})")
                        
                        # Update nhÆ° bÃ¬nh thÆ°á»ng
                        old_embedding = self.speaker_memory[matched_speaker_id]
                        updated_embedding = (1 - self.embedding_update_weight) * old_embedding + \
                                           self.embedding_update_weight * new_embedding
                        if self.embedding_metric == "cosine":
                            updated_embedding = updated_embedding / np.linalg.norm(updated_embedding)
                        self.speaker_memory[matched_speaker_id] = updated_embedding
                        
                        cluster = self.speaker_embedding_clusters[matched_speaker_id]
                        cluster.append(new_embedding.copy())
                        if len(cluster) > self.max_cluster_size:
                            self.speaker_embedding_clusters[matched_speaker_id] = cluster[-self.max_cluster_size:]
                        
                        self.speaker_counts[matched_speaker_id] += 1
                        print(f"  ğŸ“Š Updated: EMA + added to cluster (size: {len(self.speaker_embedding_clusters[matched_speaker_id])})")
                    else:
                        # Fallback: assign to first speaker (shouldn't happen)
                        fallback_speaker_id = memory_speaker_ids[0]
                        mapping[label] = fallback_speaker_id
                        print(f"  âš ï¸  Fallback: assigned to {fallback_speaker_id}")
                else:
                    # ChÆ°a Ä‘áº¡t max_speakers - Táº¡o speaker má»›i
                    speaker_id = f"SPEAKER_{self.next_speaker_id:02d}"
                    self.next_speaker_id += 1
                    mapping[label] = speaker_id
                    
                    # Initialize EMA vÃ  cluster
                    self.speaker_memory[speaker_id] = new_embedding.copy()
                    self.speaker_embedding_clusters[speaker_id] = [new_embedding.copy()]
                    self.speaker_counts[speaker_id] = 1
                    print(f"  ğŸ†• Created new speaker: {speaker_id} (current total: {len(self.speaker_memory)})")
                
        return mapping
    
    def apply_realtime(self, file_or_audio_f32: Union[str, np.ndarray], 
                      use_memory: bool = True,
                      max_speakers: Optional[int] = None,
                      **kwargs) -> dict:
        """
        Apply diarization vá»›i context memory cho realtime processing
        
        Parameters
        ----------
        file_or_audio_f32 : Union[str, np.ndarray]
            Audio chunk hiá»‡n táº¡i hoáº·c file path
        use_memory : bool
            CÃ³ sá»­ dá»¥ng speaker memory hay khÃ´ng (False = xá»­ lÃ½ nhÆ° batch bÃ¬nh thÆ°á»ng)
        max_speakers : int, optional
            Maximum number of speakers. If reached, will force-assign to best match.
        
        Returns
        -------
        output : dict
            Káº¿t quáº£ diarization vá»›i speaker labels Ä‘Ã£ Ä‘Æ°á»£c map vá»›i memory
        """
        # Gá»i phÆ°Æ¡ng thá»©c apply gá»‘c
        output = {}

        if isinstance(file_or_audio_f32, str):
            audio_f32 = decode_audio(file_or_audio_f32)
        else:
            audio_f32 = file_or_audio_f32

         # === TrÃ­ch embedding ===
        # SpeechBrain yÃªu cáº§u tensor shape (batch, time)
        try:
            tensor = torch.tensor(audio_f32).unsqueeze(0)
            with torch.no_grad():
                emb = self.spk_model.encode_batch(tensor).detach().cpu().numpy().mean(axis=1)[0]
        except Exception:
            return output
    
        # Normalize embedding Ä‘á»ƒ á»•n Ä‘á»‹nh hÆ¡n
        emb_norm = emb / (np.linalg.norm(emb) + 1e-8)
        emb_norm = np.expand_dims(emb_norm, axis=0)
        output['speaker_embeddings'] = emb_norm
        output['speaker_labels'] = ['SPEAKER_00']


        if not use_memory or output['speaker_embeddings'] is None:
            return output
        
        # Extract embeddings vÃ  labels tá»« output
        current_embeddings = output['speaker_embeddings']  # (num_speakers, dimension)
        current_labels = output['speaker_labels']
        
        
        # Match vá»›i speakers trong memory (pass max_speakers constraint)
        label_mapping = self._match_speakers_with_memory(
            current_embeddings, 
            current_labels,
            max_speakers=max_speakers
        )
        print(f"Label mapping: {label_mapping}")
        
        # Cáº­p nháº­t embeddings theo thá»© tá»± labels má»›i
        new_labels_ordered = list(label_mapping.values())
        print(f"New labels ordered: {new_labels_ordered}")
        updated_embeddings = np.array([self.speaker_memory[label] for label in new_labels_ordered])
        
        output['speaker_labels'] = new_labels_ordered
        output['speaker_embeddings'] = updated_embeddings

        # LÆ°u vÃ o history
        self.speaker_history.append({
            'chunk_id': self.total_chunks_processed,
            'labels': new_labels_ordered,
            'num_speakers': len(new_labels_ordered)
        })
        self.total_chunks_processed += 1
        
        return output
    
    def get_speaker_info(self) -> Dict:
        """Láº¥y thÃ´ng tin vá» cÃ¡c speakers Ä‘Ã£ biáº¿t"""
        cluster_sizes = {
            sid: len(self.speaker_embedding_clusters.get(sid, []))
            for sid in self.speaker_memory.keys()
        }
        return {
            'speakers': list(self.speaker_memory.keys()),
            'speaker_counts': self.speaker_counts.copy(),
            'cluster_sizes': cluster_sizes,
            'total_chunks': self.total_chunks_processed,
            'num_speakers': len(self.speaker_memory)
        }
    
    def __call__(self, file_or_audio_f32: Union[str, np.ndarray], num_speakers=None, min_speakers=None, max_speakers=None, 
              use_memory=True, **kwargs):
        """Override apply Ä‘á»ƒ sá»­ dá»¥ng realtime mode máº·c Ä‘á»‹nh"""
        # Determine effective max_speakers
        effective_max_speakers = num_speakers if num_speakers is not None else max_speakers
        
        return self.apply_realtime(
            file_or_audio_f32, 
            use_memory=use_memory, 
            num_speakers=num_speakers, 
            min_speakers=min_speakers, 
            max_speakers=effective_max_speakers,  # Pass effective limit
            **kwargs
        )

# ============ EXAMPLE USAGE ============
if __name__ == "__main__":
    # Khá»Ÿi táº¡o realtime pipeline
    pipeline = RealtimeSpeakerDiarization(
        similarity_threshold=0.7,  # threshold Ä‘á»ƒ match speaker (cÃ ng cao cÃ ng strict)
        embedding_update_weight=0.3,  # trá»ng sá»‘ update embedding (0.3 = 30% má»›i, 70% cÅ©)
        min_similarity_gap=0.3  # gap tá»‘i thiá»ƒu Ä‘á»ƒ match náº¿u ná»•i báº­t hÆ¡n háº³n
    )

    # VÃ­ dá»¥ 2: Xá»­ lÃ½ chunk thá»© 2 - speakers sáº½ Ä‘Æ°á»£c match vá»›i chunk 1
    print("\n" + "=" * 100)
    print("VÃ Dá»¤ 2: Xá»­ lÃ½ audio chunk thá»© 2 (giá»¯ context)")
    print("=" * 100)

    output2 = pipeline(
        "wav/A1.wav",
        # min_speakers=1,
        # max_speakers=3,
        num_speakers=2,
        use_memory=True
    )

    print("\nğŸ“Š Káº¿t quáº£ chunk 2:")
    print(f"  ğŸ¤ {output2['speaker_labels']}")

    print(f"\nğŸ’¾ Speaker Memory (updated): {pipeline.get_speaker_info()}")
    print("=" * 100)
    print("=" * 100)

    output3 = pipeline(
        "wav/B1.wav",
        # min_speakers=1,
        # max_speakers=3,
        num_speakers=2,
        use_memory=True
    )

    print("\nğŸ“Š Káº¿t quáº£ chunk 3:")
    print(f"  ğŸ¤ {output3['speaker_labels']}")

    print(f"\nğŸ’¾ Speaker Memory (updated): {pipeline.get_speaker_info()}")
    print("=" * 100)
    print("=" * 100)

    output4 = pipeline(
        "wav/A2.wav",
        # min_speakers=1,
        # max_speakers=3,
        num_speakers=2,
        use_memory=True
    )

    print("\nğŸ“Š Káº¿t quáº£ chunk 4:")
    print(f"  ğŸ¤ {output4['speaker_labels']}")

    print(f"\nğŸ’¾ Speaker Memory (updated): {pipeline.get_speaker_info()}")
    print("=" * 100)
    print("=" * 100)

    output5 = pipeline(
        "wav/B2.wav",
        # min_speakers=1,
        # max_speakers=3,
        num_speakers=2,
        use_memory=True
    )

    print("\nğŸ“Š Káº¿t quáº£ chunk 5:")
    print(f"  ğŸ¤ {output5['speaker_labels']}")

    print(f"\nğŸ’¾ Speaker Memory (updated): {pipeline.get_speaker_info()}")
    print("=" * 100)
    print("=" * 100)

    output6 = pipeline(
        "wav/A3.wav",
        # min_speakers=1,
        # max_speakers=3,
        num_speakers=2,
        use_memory=True
    )

    print("\nğŸ“Š Káº¿t quáº£ chunk 6:")
    print(f"  ğŸ¤ {output6['speaker_labels']}")

    print(f"\nğŸ’¾ Speaker Memory (updated): {pipeline.get_speaker_info()}")
    print("=" * 100)
    print("=" * 100)

    output7 = pipeline(
        "wav/B3.wav",
        # min_speakers=1,
        # max_speakers=3,
        num_speakers=2,
        use_memory=True
        )
    print("\nğŸ“Š Káº¿t quáº£ chunk 7:")
    print(f"  ğŸ¤ {output7['speaker_labels']}")

    print(f"\nğŸ’¾ Speaker Memory (updated): {pipeline.get_speaker_info()}")
    print("=" * 100)
    print("=" * 100)

    # VÃ­ dá»¥ 3: Reset context vÃ  báº¯t Ä‘áº§u conversation má»›i
    print("\n" + "=" * 60)
    print("VÃ Dá»¤ 3: Reset context vÃ  báº¯t Ä‘áº§u láº¡i")
    print("=" * 60)

    pipeline.reset_context()
    print(f"âœ… Context Ä‘Ã£ reset: {pipeline.get_speaker_info()}")

    # VÃ­ dá»¥ 4: Disable memory (xá»­ lÃ½ nhÆ° batch mode bÃ¬nh thÆ°á»ng)
    print("\n" + "=" * 60)
    print("VÃ Dá»¤ 4: Xá»­ lÃ½ khÃ´ng dÃ¹ng memory (batch mode)")
    print("=" * 60)

    output9 = pipeline(
        # "/home/hoang/realtime-transcript/backend/eval/TestJ.mp3",
        "wav/A1.wav",
        # min_speakers=1,
        # max_speakers=3,
        num_speakers=2,
        use_memory=False  # KhÃ´ng dÃ¹ng memory
    )

    print("\nğŸ“Š Káº¿t quáº£ batch mode:")
    print(f"  ğŸ¤ {output9['speaker_labels']}")